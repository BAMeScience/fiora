{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:07:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:07:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:07:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "# Load Modules\n",
    "sys.path.append(\"..\")\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "from fiora.MOL.constants import DEFAULT_PPM, PPM, DEFAULT_MODES\n",
    "from fiora.IO.LibraryLoader import LibraryLoader\n",
    "from fiora.MOL.FragmentationTree import FragmentationTree \n",
    "import fiora.visualization.spectrum_visualizer as sv\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "print(f'Working with Python {sys.version}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing NIST/MSDIAL library\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "lib: Literal[\"NIST\", \"MSDIAL\", \"NIST/MSDIAL\"] = \"NIST/MSDIAL\"\n",
    "print(f\"Preparing {lib} library\")\n",
    "\n",
    "test_run = False # Default: False\n",
    "if test_run:\n",
    "    print(\"+++ This is a test run with a small subset of data points. Results are not representative. +++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key map to read metadata from pandas DataFrame\n",
    "metadata_key_map = {\n",
    "                \"name\": \"Name\",\n",
    "                \"collision_energy\":  \"CE\", \n",
    "                \"instrument\": \"Instrument_type\",\n",
    "                \"ionization\": \"Ionization\",\n",
    "                \"precursor_mz\": \"PrecursorMZ\",\n",
    "                \"precursor_mode\": \"Precursor_type\",\n",
    "                \"retention_time\": \"RETENTIONTIME\",\n",
    "                \"ccs\": \"CCS\"\n",
    "                }\n",
    "\n",
    "\n",
    "#\n",
    "# Load specified libraries and align metadata\n",
    "#\n",
    "\n",
    "def load_training_data():\n",
    "    L = LibraryLoader()\n",
    "    df = L.load_from_csv(f\"{home}/data/metabolites/preprocessed/datasplits_Jan24.csv\")\n",
    "    return df\n",
    "\n",
    "df = load_training_data()\n",
    "\n",
    "# Restore dictionary values\n",
    "dict_columns = [\"peaks\", \"summary\"]\n",
    "for col in dict_columns:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x.replace('nan', 'None')))\n",
    "    #df[col] = df[col].apply(ast.literal_eval)\n",
    "    \n",
    "df['group_id'] = df['group_id'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# pdf = pl.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from fiora.MOL.Metabolite import Metabolite\n",
    "from fiora.GNN.AtomFeatureEncoder import AtomFeatureEncoder\n",
    "from fiora.GNN.BondFeatureEncoder import BondFeatureEncoder\n",
    "from fiora.GNN.SetupFeatureEncoder import SetupFeatureEncoder\n",
    "\n",
    "\n",
    "CE_upper_limit = 100.0\n",
    "weight_upper_limit = 1000.0\n",
    "\n",
    "\n",
    "if test_run:\n",
    "    df = df.iloc[:10000,:]\n",
    "    #df = df.iloc[5000:20000,:]\n",
    "\n",
    "\n",
    "df[\"Metabolite\"] = df[\"SMILES\"].apply(Metabolite)\n",
    "df[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "node_encoder = AtomFeatureEncoder(feature_list=[\"symbol\", \"num_hydrogen\", \"ring_type\"])\n",
    "bond_encoder = BondFeatureEncoder(feature_list=[\"bond_type\", \"ring_type\"])\n",
    "setup_encoder = SetupFeatureEncoder(feature_list=[\"collision_energy\", \"molecular_weight\", \"precursor_mode\", \"instrument\"])\n",
    "rt_encoder = SetupFeatureEncoder(feature_list=[\"molecular_weight\", \"precursor_mode\", \"instrument\"])\n",
    "\n",
    "setup_encoder.normalize_features[\"collision_energy\"][\"max\"] = CE_upper_limit \n",
    "setup_encoder.normalize_features[\"molecular_weight\"][\"max\"] = weight_upper_limit \n",
    "rt_encoder.normalize_features[\"molecular_weight\"][\"max\"] = weight_upper_limit \n",
    "\n",
    "df[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))\n",
    "df.apply(lambda x: x[\"Metabolite\"].set_id(x[\"group_id\"]) , axis=1)\n",
    "\n",
    "#df[\"summary\"] = df.apply(lambda x: {key: x[name] for key, name in metadata_key_map.items()}, axis=1)\n",
    "df.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "df.apply(lambda x: x[\"Metabolite\"].set_loss_weight(x[\"loss_weight\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df[\"Metabolite\"].apply(lambda x: x.fragment_MOL(depth=1))\n",
    "df.apply(lambda x: x[\"Metabolite\"].match_fragments_to_peaks(x[\"peaks\"][\"mz\"], x[\"peaks\"][\"intensity\"], tolerance=x[\"ppm_peak_tolerance\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df[\"num_peak_matches\"] = df[\"Metabolite\"].apply(lambda x: x.match_stats[\"num_peak_matches\"])\n",
    "print(sum(df[\"num_peak_matches\"] < 1))\n",
    "df = df[df[\"num_peak_matches\"] >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Casmi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "casmi16_path = f\"{home}/data/metabolites/CASMI_2016/casmi16_withCCS.csv\"\n",
    "casmi22_path = f\"{home}/data/metabolites/CASMI_2022/casmi22_withCCS.csv\"\n",
    "\n",
    "df_cas = pd.read_csv(casmi16_path, index_col=[0], low_memory=False)\n",
    "df_cas22 = pd.read_csv(casmi22_path, index_col=[0], low_memory=False)\n",
    "\n",
    "# Restore dictionary values\n",
    "dict_columns = [\"peaks\", \"Candidates\"]\n",
    "for col in dict_columns:\n",
    "    df_cas[col] = df_cas[col].apply(ast.literal_eval)\n",
    "\n",
    "df_cas22[\"peaks\"] = df_cas22[\"peaks\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from fiora.MOL.collision_energy import NCE_to_eV\n",
    "\n",
    "df_cas[\"RETENTIONTIME\"] = df_cas[\"RTINSECONDS\"] / 60.0\n",
    "df_cas[\"Metabolite\"] = df_cas[\"SMILES\"].apply(Metabolite)\n",
    "df_cas[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "df_cas[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))\n",
    "df_cas[\"CE\"] = 20.0 # actually stepped 20/35/50\n",
    "df_cas[\"Instrument_type\"] = \"HCD\" # CHECK if correct Orbitrap\n",
    "\n",
    "metadata_key_map16 = {\"collision_energy\":  \"CE\", \n",
    "                 \"instrument\": \"Instrument_type\",\n",
    "                 \"precursor_mz\": \"PRECURSOR_MZ\",\n",
    "                 'precursor_mode': \"Precursor_type\",\n",
    "                 \"retention_time\": \"RETENTIONTIME\"\n",
    "                 }\n",
    "\n",
    "df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder), axis=1)\n",
    "\n",
    "# Fragmentation\n",
    "df_cas[\"Metabolite\"].apply(lambda x: x.fragment_MOL(depth=1))\n",
    "df_cas.apply(lambda x: x[\"Metabolite\"].match_fragments_to_peaks(x[\"peaks\"][\"mz\"], x[\"peaks\"][\"intensity\"], tolerance=100 * PPM), axis=1) # Optional: use mz_cut instead\n",
    "\n",
    "#\n",
    "# CASMI 22\n",
    "#\n",
    "\n",
    "df_cas22[\"Metabolite\"] = df_cas22[\"SMILES\"].apply(Metabolite)\n",
    "df_cas22[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "df_cas22[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))\n",
    "df_cas22[\"CE\"] = df_cas22.apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"precursor_mz\"]), axis=1)\n",
    "\n",
    "metadata_key_map22 = {\"collision_energy\":  \"CE\", \n",
    "                 \"instrument\": \"Instrument_type\",\n",
    "                 \"precursor_mz\": \"precursor_mz\",\n",
    "                 'precursor_mode': \"Precursor_type\",\n",
    "                 \"retention_time\": \"ChallengeRT\"\n",
    "                 }\n",
    "\n",
    "df_cas22[\"summary\"] = df_cas22.apply(lambda x: {key: x[name] for key, name in metadata_key_map22.items()}, axis=1)\n",
    "df_cas22.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "\n",
    "# Fragmentation\n",
    "df_cas22[\"Metabolite\"].apply(lambda x: x.fragment_MOL(depth=1))\n",
    "df_cas22.apply(lambda x: x[\"Metabolite\"].match_fragments_to_peaks(x[\"peaks\"][\"mz\"], x[\"peaks\"][\"intensity\"], tolerance=100 * PPM), axis=1) # Optional: use mz_cut instead\n",
    "\n",
    "df_cas22 = df_cas22.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from fiora.GNN.Trainer import Trainer\n",
    "import torch_geometric as geom\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.empty_cache()\n",
    "    dev = \"cuda:0\"\n",
    "else: \n",
    "    dev = \"cpu\" \n",
    " \n",
    "print(f\"Running on device: {dev}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "test          1070\n",
      "training      8552\n",
      "validation    1070\n",
      "Name: group_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(\"dataset\")[\"group_id\"].unique().apply(len))\n",
    "\n",
    "df_test = df[df[\"dataset\"] == \"test\"]\n",
    "df_train = df[df[\"dataset\"].isin([\"training\", \"validation\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared training/validation with 66963 data points\n"
     ]
    }
   ],
   "source": [
    "geo_data = df_train[\"Metabolite\"].apply(lambda x: x.as_geometric_data().to(dev)).values\n",
    "print(f\"Prepared training/validation with {len(geo_data)} data points\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'param_tag': 'default',\n",
    "    'gnn_type': 'RGCNConv',\n",
    "    'depth': 6,\n",
    "    'hidden_dimension': 300,\n",
    "    'dense_layers': 2,\n",
    "    'embedding_aggregation': 'concat',\n",
    "    'embedding_dimension': 300,\n",
    "    'input_dropout': 0.2,\n",
    "    'latent_dropout': 0.1,\n",
    "    'node_feature_layout': node_encoder.feature_numbers,\n",
    "    'edge_feature_layout': bond_encoder.feature_numbers,    \n",
    "    'static_feature_dimension': geo_data[0][\"static_edge_features\"].shape[1],\n",
    "    'static_rt_feature_dimension': geo_data[0][\"static_rt_features\"].shape[1],\n",
    "    'output_dimension': len(DEFAULT_MODES) * 2, # per edge \n",
    "    \n",
    "    # Keep track of encoded features\n",
    "    'atom_features': node_encoder.feature_list,\n",
    "    'atom_features': bond_encoder.feature_list,\n",
    "    'setup_features': setup_encoder.feature_list,\n",
    "    'rt_features': rt_encoder.feature_list,\n",
    "    \n",
    "}\n",
    "training_params = {\n",
    "    'epochs': 200 if not test_run else 10, \n",
    "    'batch_size': 256,\n",
    "    #'train_val_split': 0.90,\n",
    "    'learning_rate': 0.0004, #0.001,\n",
    "    'with_RT': False, # TODO\n",
    "    'with_CCS': False\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.GNN.GNNModules import GNNCompiler\n",
    "from fiora.GNN.Losses import WeightedMSELoss, WeightedMSEMetric\n",
    "from fiora.MS.SimulationFramework import SimulationFramework\n",
    "\n",
    "fiora = SimulationFramework(None, dev=dev, with_RT=training_params[\"with_RT\"], with_CCS=training_params[\"with_CCS\"])\n",
    "# fiora = SimulationFramework(None, dev=dev, with_RT=training_params[\"with_RT\"], with_CCS=training_params[\"with_CCS\"])\n",
    "np.seterr(invalid='ignore')\n",
    "tag = \"training\"\n",
    "val_interval = 1\n",
    "metric_dict= {\"mse\": WeightedMSEMetric}\n",
    "loss_fn = WeightedMSELoss()\n",
    "all_together = False\n",
    "\n",
    "if all_together:\n",
    "    val_interval = 200\n",
    "    metric_dict=None\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def train_new_model():\n",
    "    model = GNNCompiler(model_params).to(dev)\n",
    "        \n",
    "    y_label = 'compiled_probsALL'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=training_params[\"learning_rate\"])\n",
    "    if all_together:\n",
    "        trainer = Trainer(geo_data, y_tag=y_label, problem_type=\"regression\", only_training=True, metric_dict=metric_dict, split_by_group=True, seed=seed, device=dev)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    else:\n",
    "        train_keys, val_keys = df[df[\"dataset\"] == \"training\"][\"group_id\"].unique(), df[df[\"dataset\"] == \"validation\"][\"group_id\"].unique()\n",
    "        trainer = Trainer(geo_data, y_tag=y_label, problem_type=\"regression\", train_keys=train_keys, val_keys=val_keys, metric_dict=metric_dict, split_by_group=True, seed=seed, device=dev)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 8, factor=0.5, mode = 'min', verbose = True)\n",
    "\n",
    "    \n",
    "    checkpoints = trainer.train(model, optimizer, loss_fn, scheduler=scheduler, batch_size=training_params['batch_size'], epochs=training_params[\"epochs\"], val_every_n_epochs=1, with_CCS=training_params[\"with_CCS\"], with_RT=training_params[\"with_RT\"], masked_validation=False, tag=tag) #, mask_name=\"compiled_validation_maskALL\")   \n",
    "    \n",
    "    return model, checkpoints\n",
    "\n",
    "def simulate_all(model, DF):\n",
    "    return fiora.simulate_all(DF, model)\n",
    "\n",
    "    \n",
    "def test_model(model, DF):\n",
    "    dft = simulate_all(model, DF)\n",
    "    \n",
    "    return dft[\"spectral_sqrt_cosine\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test CASMI 16 and 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.MOL.collision_energy import NCE_to_eV\n",
    "from fiora.MS.spectral_scores import spectral_cosine, spectral_reflection_cosine, reweighted_dot\n",
    "from fiora.MS.ms_utility import merge_annotated_spectrum\n",
    "\n",
    "def test_cas16(model, df_cas=df_cas):\n",
    "    \n",
    "    df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
    "    df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
    "    df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
    "    df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "    df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "    df_cas = fiora.simulate_all(df_cas, model, suffix=\"_20\")\n",
    "\n",
    "    df_cas[\"NCE\"] = 35.0 # actually stepped NCE 20/35/50\n",
    "    df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
    "    df_cas[\"step2_CE\"] = df_cas[\"CE\"]\n",
    "    df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "    df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "    df_cas = fiora.simulate_all(df_cas, model, suffix=\"_35\")\n",
    "\n",
    "\n",
    "    df_cas[\"NCE\"] = 50.0 # actually stepped NCE 20/35/50\n",
    "    df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
    "    df_cas[\"step3_CE\"] = df_cas[\"CE\"]\n",
    "    df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "    df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "    df_cas = fiora.simulate_all(df_cas, model, suffix=\"_50\")\n",
    "\n",
    "    df_cas[\"avg_CE\"] = (df_cas[\"step1_CE\"] + df_cas[\"step2_CE\"] + df_cas[\"step3_CE\"]) / 3\n",
    "\n",
    "    df_cas[\"merged_peaks\"] = df_cas.apply(lambda x: merge_annotated_spectrum(merge_annotated_spectrum(x[\"sim_peaks_20\"], x[\"sim_peaks_35\"]), x[\"sim_peaks_50\"]) , axis=1)\n",
    "    df_cas[\"merged_cosine\"] = df_cas.apply(lambda x: spectral_cosine(x[\"peaks\"], x[\"merged_peaks\"]), axis=1)\n",
    "    df_cas[\"merged_sqrt_cosine\"] = df_cas.apply(lambda x: spectral_cosine(x[\"peaks\"], x[\"merged_peaks\"], transform=np.sqrt), axis=1)\n",
    "    df_cas[\"merged_refl_cosine\"] = df_cas.apply(lambda x: spectral_reflection_cosine(x[\"peaks\"], x[\"merged_peaks\"], transform=np.sqrt), axis=1)\n",
    "    df_cas[\"merged_steins\"] = df_cas.apply(lambda x: reweighted_dot(x[\"peaks\"], x[\"merged_peaks\"]), axis=1)\n",
    "    df_cas[\"spectral_sqrt_cosine\"] = df_cas[\"merged_sqrt_cosine\"] # just remember it is merged\n",
    "\n",
    "    df_cas[\"coverage\"] = df_cas[\"Metabolite\"].apply(lambda x: x.match_stats[\"coverage\"])\n",
    "    df_cas[\"RT_pred\"] = df_cas[\"RT_pred_35\"]\n",
    "    df_cas[\"RT_dif\"] = df_cas[\"RT_dif_35\"]\n",
    "    df_cas[\"CCS_pred\"] = df_cas[\"CCS_pred_35\"]\n",
    "    df_cas[\"library\"] = \"CASMI-16\"\n",
    "    \n",
    "    return df_cas[\"merged_sqrt_cosine\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-arranged train/validation set\n",
      "\tValidation RMSE: 0.011116\n",
      "\tValidation RMSE: 0.010707\n",
      "\tValidation RMSE: 0.010402\n",
      "\tValidation RMSE: 0.009998\n",
      "\tValidation RMSE: 0.009795\n",
      "\tValidation RMSE: 0.009292\n",
      "\tValidation RMSE: 0.009490\n",
      "\tValidation RMSE: 0.009288\n",
      "\tValidation RMSE: 0.008986\n",
      "\tValidation RMSE: 0.0090085\n",
      "\tValidation RMSE: 0.0092084\n",
      "\tValidation RMSE: 0.0091082\n",
      "\tValidation RMSE: 0.0089082\n",
      "\tValidation RMSE: 0.0090080\n",
      "\tValidation RMSE: 0.0088079\n",
      "\tValidation RMSE: 0.0087078\n",
      "\tValidation RMSE: 0.0087077\n",
      "\tValidation RMSE: 0.0087076\n",
      "\tValidation RMSE: 0.0087076\n",
      "\tValidation RMSE: 0.0087075\n",
      "\tValidation RMSE: 0.0087074\n",
      "\tValidation RMSE: 0.0086073\n",
      "\tValidation RMSE: 0.0086073\n",
      "\tValidation RMSE: 0.0086072\n",
      "\tValidation RMSE: 0.0088072\n",
      "\tValidation RMSE: 0.0086071\n",
      "\tValidation RMSE: 0.0086070\n",
      "\tValidation RMSE: 0.0086069\n",
      "\tValidation RMSE: 0.0085069\n",
      "\tValidation RMSE: 0.0086069\n",
      "\tValidation RMSE: 0.0086068\n",
      "\tValidation RMSE: 0.0087068\n",
      "\tValidation RMSE: 0.0084067\n",
      "\tValidation RMSE: 0.0085066\n",
      "\tValidation RMSE: 0.0085066\n",
      "\tValidation RMSE: 0.0085065\n",
      "\tValidation RMSE: 0.0086065\n",
      "\tValidation RMSE: 0.0083065\n",
      "\tValidation RMSE: 0.0084064\n",
      "\tValidation RMSE: 0.0085064\n",
      "\tValidation RMSE: 0.0085063\n",
      "\tValidation RMSE: 0.0085063\n",
      "\tValidation RMSE: 0.0084063\n",
      "\tValidation RMSE: 0.0084062\n",
      "\tValidation RMSE: 0.0086062\n",
      "\tValidation RMSE: 0.0085061\n",
      "\tValidation RMSE: 0.0084061\n",
      "Epoch 00047: reducing learning rate of group 0 to 2.0000e-04.\n",
      "\tValidation RMSE: 0.0083058\n",
      "\tValidation RMSE: 0.0084058\n",
      "\tValidation RMSE: 0.0083057\n",
      "\tValidation RMSE: 0.0084057\n",
      "\tValidation RMSE: 0.0083057\n",
      "\tValidation RMSE: 0.0083056\n",
      "\tValidation RMSE: 0.0084056\n",
      "\tValidation RMSE: 0.0083056\n",
      "\tValidation RMSE: 0.0083056\n",
      "\tValidation RMSE: 0.0083056\n",
      "Epoch 00057: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\tValidation RMSE: 0.0083054\n",
      "\tValidation RMSE: 0.0083054\n",
      "\tValidation RMSE: 0.0083054\n",
      "\tValidation RMSE: 0.0083053\n",
      "\tValidation RMSE: 0.0082053\n",
      "\tValidation RMSE: 0.0083053\n",
      "\tValidation RMSE: 0.0082053\n",
      "\tValidation RMSE: 0.0082053\n",
      "\tValidation RMSE: 0.0083053\n",
      "\tValidation RMSE: 0.0083053\n",
      "\tValidation RMSE: 0.0083053\n",
      "\tValidation RMSE: 0.0083052\n",
      "\tValidation RMSE: 0.0083052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0083052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0083052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0082052\n",
      "\tValidation RMSE: 0.0083051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0083051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082051\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.0082050\n",
      "\tValidation RMSE: 0.00820050\n",
      "\tValidation RMSE: 0.00820050\n",
      "\tValidation RMSE: 0.00820050\n",
      "\tValidation RMSE: 0.00820050\n",
      "Epoch 00103: reducing learning rate of group 0 to 5.0000e-05.\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00810049\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820049\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00810048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "Epoch 00127: reducing learning rate of group 0 to 2.5000e-05.\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820048\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00810047\n",
      "Epoch 00136: reducing learning rate of group 0 to 1.2500e-05.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00145: reducing learning rate of group 0 to 6.2500e-06.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00154: reducing learning rate of group 0 to 3.1250e-06.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00163: reducing learning rate of group 0 to 1.5625e-06.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00172: reducing learning rate of group 0 to 7.8125e-07.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00181: reducing learning rate of group 0 to 3.9063e-07.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00190: reducing learning rate of group 0 to 1.9531e-07.\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "\tValidation RMSE: 0.00820047\n",
      "Epoch 00199: reducing learning rate of group 0 to 9.7656e-08.\n",
      "\tValidation RMSE: 0.00820047\n",
      "Finished Training!\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "print(f\"Training model\")\n",
    "model, checkpoints = train_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 136,\n",
       " 'val_loss': 6.631344876950607e-05,\n",
       " 'file': '../../checkpoint_training.best.pt'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ynowatzk/repos/fiora/fiora/MS/SimulationFramework.py:164: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame(columns=[x + suffix for x in [\"cosine_similarity\", \"kl_div\", \"sim_peaks\", \"spectral_cosine\", \"spectral_sqrt_cosine\", \"spectral_sqrt_cosine_wo_prec\", \"spectral_refl_cosine\", \"spectral_bias\", \"spectral_sqrt_bias\", \"spectral_sqrt_bias_wo_prec\", \"spectral_refl_bias\", \"steins_cosine\", \"steins_bias\", \"RT_pred\", \"RT_dif\", \"CCS_pred\"]])])\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/SimulationFramework.py:164: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame(columns=[x + suffix for x in [\"cosine_similarity\", \"kl_div\", \"sim_peaks\", \"spectral_cosine\", \"spectral_sqrt_cosine\", \"spectral_sqrt_cosine_wo_prec\", \"spectral_refl_cosine\", \"spectral_bias\", \"spectral_sqrt_bias\", \"spectral_sqrt_bias_wo_prec\", \"spectral_refl_bias\", \"steins_cosine\", \"steins_bias\", \"RT_pred\", \"RT_dif\", \"CCS_pred\"]])])\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/tmp/ipykernel_3419651/4223613324.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
      "/tmp/ipykernel_3419651/4223613324.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
      "/tmp/ipykernel_3419651/4223613324.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
      "/tmp/ipykernel_3419651/4223613324.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/tmp/ipykernel_3419651/4223613324.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
      "/tmp/ipykernel_3419651/4223613324.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
      "/tmp/ipykernel_3419651/4223613324.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
      "/tmp/ipykernel_3419651/4223613324.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec = vec / np.linalg.norm(vec)\n",
      "/home/ynowatzk/repos/fiora/fiora/MS/spectral_scores.py:27: RuntimeWarning: divide by zero encountered in divide\n",
      "  vec_other = vec_other / np.linalg.norm(vec_other)\n"
     ]
    }
   ],
   "source": [
    "model = GNNCompiler.load(checkpoints[\"file\"])\n",
    "val_results = test_model(model, df_train[df_train[\"dataset\"]== \"validation\"])\n",
    "test_results = test_model(model, df_test)\n",
    "casmi16_results = test_cas16(model)\n",
    "casmi16_p = test_cas16(model, df_cas[df_cas[\"Precursor_type\"] == \"[M+H]+\"])\n",
    "casmi16_n = test_cas16(model, df_cas[df_cas[\"Precursor_type\"] == \"[M-H]-\"])\n",
    "casmi22_results = test_model(model, df_cas22)\n",
    "casmi22_p = test_model(model, df_cas22[df_cas22[\"Precursor_type\"] == \"[M+H]+\"])\n",
    "casmi22_n = test_model(model, df_cas22[df_cas22[\"Precursor_type\"] == \"[M-H]-\"])\n",
    "    \n",
    "results.append({\"model\": model, \"validation\": val_results, \"test\": test_results, \"casmi16\": casmi16_results, \"casmi22\": casmi22_results, \"casmi16+\": casmi16_p, \"casmi16-\": casmi16_n, \"casmi22+\": casmi22_p, \"casmi22-\": casmi22_n})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG = pd.DataFrame(results)\n",
    "# eval_columns = LOG.columns[3:]\n",
    "\n",
    "# home_path = f\"{home}/data/metabolites/benchmarking/\"\n",
    "# NAME = model_params[\"gnn_type\"] + \"_depth.csv\"\n",
    "# for col in eval_columns:\n",
    "#     LOG[col] = LOG[col].apply(lambda x: str(list(x)))\n",
    "# LOG.to_csv(home_path + NAME, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGIC = pd.read_csv(home_path + NAME, sep=\"\\t\")\n",
    "# for col in eval_columns:\n",
    "#     LOGIC[col] = LOGIC[col].apply(lambda x: ast.literal_eval(x.replace('nan', 'None')))\n",
    "\n",
    "# LOGIC[eval_columns] = LOGIC[eval_columns].apply(lambda x: x.apply(np.median))\n",
    "# LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>casmi16</th>\n",
       "      <th>casmi22</th>\n",
       "      <th>casmi16+</th>\n",
       "      <th>casmi16-</th>\n",
       "      <th>casmi22+</th>\n",
       "      <th>casmi22-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GNNCompiler(\\n  (node_embedding): FeatureEmbed...</td>\n",
       "      <td>0.81913</td>\n",
       "      <td>0.832823</td>\n",
       "      <td>0.718098</td>\n",
       "      <td>0.259683</td>\n",
       "      <td>0.701738</td>\n",
       "      <td>0.733384</td>\n",
       "      <td>0.249562</td>\n",
       "      <td>0.266537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  validation      test  \\\n",
       "0  GNNCompiler(\\n  (node_embedding): FeatureEmbed...     0.81913  0.832823   \n",
       "\n",
       "    casmi16   casmi22  casmi16+  casmi16-  casmi22+  casmi22-  \n",
       "0  0.718098  0.259683  0.701738  0.733384  0.249562  0.266537  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG = pd.DataFrame(results)\n",
    "eval_columns = LOG.columns[1:]\n",
    "LOG[eval_columns] = LOG[eval_columns].apply(lambda x: x.apply(np.median))\n",
    "\n",
    "LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = model_params[\"depth\"]\n",
    "MODEL_PATH = f\"{home}/data/metabolites/pretrained_models/v0.0.1_merged_depth{depth}_Jan24.pt\"\n",
    "model.save(MODEL_PATH)\n",
    "# print(f\"Saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.GNN.GNNModules import GNNCompiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=\"cuda:3\"\n",
    "mymy = GNNCompiler.load(MODEL_PATH) # f\"{home}/data/metabolites/pretrained_models/v0.0.1_merged_2.pt\"\n",
    "#mymy.load_state_dict(torch.load(f\"{home}/data/metabolites/pretrained_models/test.pt\"))\n",
    "mymy.eval()\n",
    "mymy = mymy.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7237091044279388"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(test_model(mymy, df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2357750642225984"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(test_model(mymy, df_cas22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(MODEL_PATH.replace(\".pt\", \"_params.json\"), 'r') as fp:\n",
    "    p = json.load(fp)\n",
    "hh = GNNCompiler(p)\n",
    "hh.load_state_dict(torch.load(MODEL_PATH.replace(\".pt\", \"_state.pt\")))\n",
    "hh.eval()\n",
    "hh = hh.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2357750642225984"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(test_model(hh, df_cas22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "TODO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ynowatzk/repos/metabolites/notebooks/train_model.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsv2111/home/ynowatzk/repos/metabolites/notebooks/train_model.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTODO\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: TODO"
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt(\"TODO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      group_id                                             SMILES  \\\n",
      "173         15  CC1(C)S[C@@H]2[C@H](NC(=O)[C@H](N)c3ccccc3)C(=...   \n",
      "447         40  CCN(CC)CCCC(C)Nc(c21)c(c3)c(ccc(OC)3)nc(cc(Cl)...   \n",
      "1172       118                           N[C@@H](Cc1ccccc1)C(=O)O   \n",
      "1224       124               NC(CCC(=O)NC(CS)C(=O)NCC(=O)O)C(=O)O   \n",
      "1266       129          O=S(=O)(c1ccc(cc1)NC(=O)C)Nc1nc(cc(n1)C)C   \n",
      "\n",
      "     Precursor_type  \n",
      "173          [M-H]-  \n",
      "447          [M-H]-  \n",
      "1172         [M-H]-  \n",
      "1224         [M-H]-  \n",
      "1266         [M-H]-  \n"
     ]
    }
   ],
   "source": [
    "## prepare output for for CFM-ID\n",
    "import os\n",
    "save_df = False\n",
    "cfm_directory = f\"{home}/data/metabolites/cfm-id/\"\n",
    "name = \"test_split_negative_solutions_cfm.txt\"\n",
    "df_cfm = df_test[[\"group_id\", \"SMILES\", \"Precursor_type\"]]\n",
    "df_n = df_cfm[df_cfm[\"Precursor_type\"] == \"[M-H]-\"].drop_duplicates(subset='group_id', keep='first')\n",
    "df_p = df_cfm[df_cfm[\"Precursor_type\"] == \"[M+H]+\"].drop_duplicates(subset='group_id', keep='first')\n",
    "\n",
    "print(df_n.head())\n",
    "\n",
    "if save_df:\n",
    "    file = os.path.join(cfm_directory, name)\n",
    "    df_n[[\"group_id\", \"SMILES\"]].to_csv(file, index=False, header=False, sep=\" \")\n",
    "    \n",
    "    name = name.replace(\"negative\", \"positive\")\n",
    "    file = os.path.join(cfm_directory, name)\n",
    "    df_p[[\"group_id\", \"SMILES\"]].to_csv(file, index=False, header=False, sep=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e08c3b4e89cfe29feb848cdc03972f3418883da099973a3eef9fa9a5a5dc99db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
