{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:52:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[10:52:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[10:52:36] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Python 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_printoptions(precision=2, sci_mode=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import copy\n",
    "\n",
    "# Load Modules\n",
    "sys.path.append(\"..\")\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "from fiora.MOL.constants import DEFAULT_PPM, PPM, DEFAULT_MODES\n",
    "from fiora.IO.LibraryLoader import LibraryLoader\n",
    "from fiora.MOL.FragmentationTree import FragmentationTree \n",
    "import fiora.visualization.spectrum_visualizer as sv\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "print(f'Working with Python {sys.version}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing MSnLib library\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "lib: Literal[\"NIST\", \"MSDIAL\", \"NIST/MSDIAL\", \"MSnLib\"] = \"MSnLib\" #\"MSnLib\"\n",
    "print(f\"Preparing {lib} library\")\n",
    "\n",
    "debug_mode = False # Default: False\n",
    "if debug_mode:\n",
    "    print(\"+++ This is a test run (debug mode) with a small subset of data points. Results are not representative. +++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key map to read metadata from pandas DataFrame\n",
    "metadata_key_map = {\n",
    "                \"name\": \"Name\",\n",
    "                \"collision_energy\":  \"CE\", \n",
    "                \"instrument\": \"Instrument_type\",\n",
    "                \"ionization\": \"Ionization\",\n",
    "                \"precursor_mz\": \"PrecursorMZ\",\n",
    "                \"precursor_mode\": \"Precursor_type\",\n",
    "                \"retention_time\": \"RETENTIONTIME\",\n",
    "                \"ccs\": \"CCS\"\n",
    "                }\n",
    "\n",
    "\n",
    "#\n",
    "# Load specified libraries and align metadata\n",
    "#\n",
    "\n",
    "def load_training_data():\n",
    "    if (\"NIST\" in lib or \"MSDIAL\" in lib):\n",
    "        data_path: str = f\"{home}/data/metabolites/preprocessed/datasplits_Jan24.csv\"\n",
    "    elif lib == \"MSnLib\":\n",
    "        data_path: str = f\"{home}/data/metabolites/preprocessed/datasplits_msnlib_April25_v1.csv\"\n",
    "    else:\n",
    "        raise NameError(f\"Unknown library selected {lib=}.\")\n",
    "    L = LibraryLoader()\n",
    "    df = L.load_from_csv(data_path)\n",
    "    return df\n",
    "\n",
    "df = load_training_data()\n",
    "\n",
    "# Restore dictionary values\n",
    "dict_columns = [\"peaks\", \"summary\"]\n",
    "for col in dict_columns:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x.replace('nan', 'None')))\n",
    "    #df[col] = df[col].apply(ast.literal_eval)\n",
    "    \n",
    "df['group_id'] = df['group_id'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# pdf = pl.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from fiora.MOL.Metabolite import Metabolite\n",
    "from fiora.GNN.AtomFeatureEncoder import AtomFeatureEncoder\n",
    "from fiora.GNN.BondFeatureEncoder import BondFeatureEncoder\n",
    "from fiora.GNN.SetupFeatureEncoder import SetupFeatureEncoder\n",
    "\n",
    "\n",
    "CE_upper_limit = 100.0\n",
    "weight_upper_limit = 1000.0\n",
    "\n",
    "\n",
    "if debug_mode:\n",
    "    df = df.iloc[:10000,:]\n",
    "    #df = df.iloc[5000:20000,:]\n",
    "\n",
    "overwrite_setup_features = None\n",
    "if lib == \"MSnLib\":\n",
    "    overwrite_setup_features = {\n",
    "        \"instrument\": [\"HCD\"],\n",
    "        \"precursor_mode\": [\"[M+H]+\", \"[M-H]-\", \"[M]+\", \"[M]-\"]\n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "df[\"Metabolite\"] = df[\"SMILES\"].apply(Metabolite)\n",
    "df[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "node_encoder = AtomFeatureEncoder(feature_list=[\"symbol\", \"num_hydrogen\", \"ring_type\"])\n",
    "bond_encoder = BondFeatureEncoder(feature_list=[\"bond_type\", \"ring_type\"])\n",
    "setup_encoder = SetupFeatureEncoder(feature_list=[\"collision_energy\", \"molecular_weight\", \"precursor_mode\", \"instrument\"], sets_overwrite=overwrite_setup_features)\n",
    "rt_encoder = SetupFeatureEncoder(feature_list=[\"molecular_weight\", \"precursor_mode\", \"instrument\"], sets_overwrite=overwrite_setup_features)\n",
    "\n",
    "setup_encoder.normalize_features[\"collision_energy\"][\"max\"] = CE_upper_limit \n",
    "setup_encoder.normalize_features[\"molecular_weight\"][\"max\"] = weight_upper_limit \n",
    "rt_encoder.normalize_features[\"molecular_weight\"][\"max\"] = weight_upper_limit \n",
    "\n",
    "df[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))\n",
    "df.apply(lambda x: x[\"Metabolite\"].set_id(x[\"group_id\"]) , axis=1)\n",
    "\n",
    "#df[\"summary\"] = df.apply(lambda x: {key: x[name] for key, name in metadata_key_map.items()}, axis=1)\n",
    "df.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "df.apply(lambda x: x[\"Metabolite\"].set_loss_weight(x[\"loss_weight\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.MOL.MetaboliteIndex import MetaboliteIndex\n",
    "\n",
    "mindex: MetaboliteIndex = MetaboliteIndex()\n",
    "mindex.index_metabolites(df[\"Metabolite\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindex.create_fragmentation_trees()\n",
    "list_of_mismatched_ids = mindex.add_fragmentation_trees_to_metabolite_list(df[\"Metabolite\"], graph_mismatch_policy=\"recompute\")\n",
    "print(f\"Total number of recomputed trees: {len(list_of_mismatched_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#df[\"Metabolite\"].apply(lambda x: x.fragment_MOL(depth=1))\n",
    "df.apply(lambda x: x[\"Metabolite\"].match_fragments_to_peaks(x[\"peaks\"][\"mz\"], x[\"peaks\"][\"intensity\"], tolerance=x[\"ppm_peak_tolerance\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df[\"num_peak_matches\"] = df[\"Metabolite\"].apply(lambda x: x.match_stats[\"num_peak_matches\"])\n",
    "print(sum(df[\"num_peak_matches\"] < 1))\n",
    "df = df[df[\"num_peak_matches\"] >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Casmi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casmi16_path = f\"{home}/data/metabolites/CASMI_2016/casmi16_withCCS.csv\"\n",
    "casmi22_path = f\"{home}/data/metabolites/CASMI_2022/casmi22_withCCS.csv\"\n",
    "\n",
    "df_cas = pd.read_csv(casmi16_path, index_col=[0], low_memory=False)\n",
    "df_cas22 = pd.read_csv(casmi22_path, index_col=[0], low_memory=False)\n",
    "\n",
    "# Restore dictionary values\n",
    "dict_columns = [\"peaks\", \"Candidates\"]\n",
    "for col in dict_columns:\n",
    "    df_cas[col] = df_cas[col].apply(ast.literal_eval)\n",
    "\n",
    "df_cas22[\"peaks\"] = df_cas22[\"peaks\"].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from fiora.MOL.collision_energy import NCE_to_eV\n",
    "\n",
    "df_cas[\"RETENTIONTIME\"] = df_cas[\"RTINSECONDS\"] / 60.0\n",
    "df_cas[\"Metabolite\"] = df_cas[\"SMILES\"].apply(Metabolite)\n",
    "df_cas[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "df_cas[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))\n",
    "df_cas[\"CE\"] = 20.0 # actually stepped 20/35/50\n",
    "df_cas[\"Instrument_type\"] = \"HCD\" # CHECK if correct Orbitrap\n",
    "\n",
    "metadata_key_map16 = {\"collision_energy\":  \"CE\", \n",
    "                 \"instrument\": \"Instrument_type\",\n",
    "                 \"precursor_mz\": \"PRECURSOR_MZ\",\n",
    "                 'precursor_mode': \"Precursor_type\",\n",
    "                 \"retention_time\": \"RETENTIONTIME\"\n",
    "                 }\n",
    "\n",
    "df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder), axis=1)\n",
    "\n",
    "# Fragmentation\n",
    "df_cas[\"Metabolite\"].apply(lambda x: x.fragment_MOL(depth=1))\n",
    "df_cas.apply(lambda x: x[\"Metabolite\"].match_fragments_to_peaks(x[\"peaks\"][\"mz\"], x[\"peaks\"][\"intensity\"], tolerance=100 * PPM), axis=1) # Optional: use mz_cut instead\n",
    "\n",
    "#\n",
    "# CASMI 22\n",
    "#\n",
    "\n",
    "df_cas22[\"Metabolite\"] = df_cas22[\"SMILES\"].apply(Metabolite)\n",
    "df_cas22[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "df_cas22[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))\n",
    "df_cas22[\"CE\"] = df_cas22.apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"precursor_mz\"]), axis=1)\n",
    "\n",
    "metadata_key_map22 = {\"collision_energy\":  \"CE\", \n",
    "                 \"instrument\": \"Instrument_type\",\n",
    "                 \"precursor_mz\": \"precursor_mz\",\n",
    "                 'precursor_mode': \"Precursor_type\",\n",
    "                 \"retention_time\": \"ChallengeRT\"\n",
    "                 }\n",
    "\n",
    "df_cas22[\"summary\"] = df_cas22.apply(lambda x: {key: x[name] for key, name in metadata_key_map22.items()}, axis=1)\n",
    "df_cas22.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "\n",
    "# Fragmentation\n",
    "df_cas22[\"Metabolite\"].apply(lambda x: x.fragment_MOL(depth=1))\n",
    "df_cas22.apply(lambda x: x[\"Metabolite\"].match_fragments_to_peaks(x[\"peaks\"][\"mz\"], x[\"peaks\"][\"intensity\"], tolerance=100 * PPM), axis=1) # Optional: use mz_cut instead\n",
    "\n",
    "df_cas22 = df_cas22.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric as geom\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.empty_cache()\n",
    "    dev = \"cuda:0\"\n",
    "else: \n",
    "    dev = \"cpu\" \n",
    " \n",
    "print(f\"Running on device: {dev}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "test           1799\n",
      "training      14392\n",
      "validation     1799\n",
      "Name: group_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby(\"dataset\")[\"group_id\"].unique().apply(len))\n",
    "\n",
    "df_test = df[df[\"dataset\"] == \"test\"]\n",
    "df_train = df[df[\"dataset\"].isin([\"training\", \"validation\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared training/validation with 63144 data points\n"
     ]
    }
   ],
   "source": [
    "geo_data = df_train[\"Metabolite\"].apply(lambda x: x.as_geometric_data().to(dev)).values\n",
    "print(f\"Prepared training/validation with {len(geo_data)} data points\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'param_tag': 'default',\n",
    "    'gnn_type': 'RGCNConv',\n",
    "    'depth': 6,\n",
    "    'hidden_dimension': 300,\n",
    "    'dense_layers': 2,\n",
    "    'embedding_aggregation': 'concat',\n",
    "    'embedding_dimension': 300,\n",
    "    'input_dropout': 0.2,\n",
    "    'latent_dropout': 0.1,\n",
    "    'node_feature_layout': node_encoder.feature_numbers,\n",
    "    'edge_feature_layout': bond_encoder.feature_numbers,    \n",
    "    'static_feature_dimension': geo_data[0][\"static_edge_features\"].shape[1],\n",
    "    'static_rt_feature_dimension': geo_data[0][\"static_rt_features\"].shape[1],\n",
    "    'output_dimension': len(DEFAULT_MODES) * 2, # per edge \n",
    "    \n",
    "    # Keep track of encoded features\n",
    "    'atom_features': node_encoder.feature_list,\n",
    "    'atom_features': bond_encoder.feature_list,\n",
    "    'setup_features': setup_encoder.feature_list,\n",
    "    'setup_features_categorical_set': setup_encoder.categorical_sets,\n",
    "    'rt_features': rt_encoder.feature_list,\n",
    "    \n",
    "    # Set default flags (May be overwritten below)\n",
    "    'rt_supported': False,\n",
    "    'ccs_supported': False,\n",
    "    'version': \"x.x.x\"\n",
    "    \n",
    "}\n",
    "training_params = {\n",
    "    'epochs': 200 if not debug_mode else 10, \n",
    "    'batch_size': 256,\n",
    "    #'train_val_split': 0.90,\n",
    "    'learning_rate': 0.0004, # 0.00001 currently for wMAE # Default for wMSE is 0.0004, #0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'with_RT': False, # Turn off RT/CCS for initial trainings round\n",
    "    'with_CCS': False\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Subsample training data\n",
    "def subsample_keys(train_keys, val_keys, down_to_fraction: float):\n",
    "    train_sample = np.random.choice(train_keys, size=int(len(train_keys) * down_to_fraction), replace=False)\n",
    "    val_sample = np.random.choice(val_keys, size=int(len(val_keys) * down_to_fraction), replace=False)\n",
    "    return train_sample, val_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.GNN.SpectralTrainer import SpectralTrainer\n",
    "from fiora.GNN.GNNModules import GNNCompiler\n",
    "from fiora.GNN.Losses import WeightedMSELoss, WeightedMSEMetric, WeightedMAELoss, WeightedMAEMetric\n",
    "from fiora.MS.SimulationFramework import SimulationFramework\n",
    "\n",
    "fiora = SimulationFramework(None, dev=dev, with_RT=True, with_CCS=True)\n",
    "# fiora = SimulationFramework(None, dev=dev, with_RT=training_params[\"with_RT\"], with_CCS=training_params[\"with_CCS\"])\n",
    "np.seterr(invalid='ignore')\n",
    "tag = \"training\"\n",
    "val_interval = 1\n",
    "metric_dict= {\"mse\": WeightedMSEMetric} #WeightedMSEMetric\n",
    "loss_fn = WeightedMSELoss() # WeightedMSELoss()\n",
    "all_together = False\n",
    "down_sample = False\n",
    "\n",
    "if all_together:\n",
    "    val_interval = 200\n",
    "    metric_dict=None\n",
    "    loss_fn = torch.nn.MSELoss()    \n",
    "\n",
    "def train_new_model(continue_with_model=None):\n",
    "    if continue_with_model:\n",
    "        model = continue_with_model.to(dev)\n",
    "    else:\n",
    "        model = GNNCompiler(model_params).to(dev)\n",
    "        \n",
    "    y_label = 'compiled_probsSQRT' # y_label = 'compiled_probsALL'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=training_params[\"learning_rate\"], weight_decay=training_params[\"weight_decay\"])\n",
    "    if all_together:\n",
    "        trainer = SpectralTrainer(geo_data, y_tag=y_label, problem_type=\"regression\", only_training=True, metric_dict=metric_dict, split_by_group=True, seed=seed, device=dev)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    else:\n",
    "        train_keys, val_keys = df[df[\"dataset\"] == \"training\"][\"group_id\"].unique(), df[df[\"dataset\"] == \"validation\"][\"group_id\"].unique()\n",
    "        if down_sample:\n",
    "            train_fraction = 0.10\n",
    "            train_keys, val_keys = subsample_keys(train_keys, val_keys, train_fraction) # Downsample training data for test\n",
    "            print(f\"Sample down to {train_fraction * 100}% with {len(train_keys)} training and {len(val_keys)} validation compounds \")\n",
    "        trainer = SpectralTrainer(geo_data, y_tag=y_label, problem_type=\"regression\", train_keys=train_keys, val_keys=val_keys, metric_dict=metric_dict, split_by_group=True, seed=seed, device=dev)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 8, factor=0.5, mode = 'min', verbose = True)\n",
    "\n",
    "    \n",
    "    checkpoints = trainer.train(model, optimizer, loss_fn, scheduler=scheduler, batch_size=training_params['batch_size'], epochs=training_params[\"epochs\"], val_every_n_epochs=1, with_CCS=training_params[\"with_CCS\"], with_RT=training_params[\"with_RT\"], use_validation_mask=False, tag=tag) #, mask_name=\"compiled_validation_maskALL\")   \n",
    "    print(checkpoints)\n",
    "    return model, checkpoints\n",
    "\n",
    "def simulate_all(model, DF):\n",
    "    return fiora.simulate_all(DF, model)\n",
    "\n",
    "    \n",
    "def test_model(model, DF, score=\"spectral_sqrt_cosine\", return_df=False):\n",
    "    dft = simulate_all(model, DF)\n",
    "    \n",
    "    if return_df:\n",
    "        return dft\n",
    "    return dft[score].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test CASMI 16 and 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.MOL.collision_energy import NCE_to_eV\n",
    "from fiora.MS.spectral_scores import spectral_cosine, spectral_reflection_cosine, reweighted_dot\n",
    "from fiora.MS.ms_utility import merge_annotated_spectrum\n",
    "\n",
    "def test_cas16(model, df_cas=df_cas, score=\"merged_sqrt_cosine\", return_df=False):\n",
    "    \n",
    "    df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
    "    df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
    "    df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
    "    df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "    df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "    df_cas = fiora.simulate_all(df_cas, model, suffix=\"_20\")\n",
    "\n",
    "    df_cas[\"NCE\"] = 35.0 # actually stepped NCE 20/35/50\n",
    "    df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
    "    df_cas[\"step2_CE\"] = df_cas[\"CE\"]\n",
    "    df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "    df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "    df_cas = fiora.simulate_all(df_cas, model, suffix=\"_35\")\n",
    "\n",
    "\n",
    "    df_cas[\"NCE\"] = 50.0 # actually stepped NCE 20/35/50\n",
    "    df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
    "    df_cas[\"step3_CE\"] = df_cas[\"CE\"]\n",
    "    df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
    "    df_cas.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "    df_cas = fiora.simulate_all(df_cas, model, suffix=\"_50\")\n",
    "\n",
    "    df_cas[\"avg_CE\"] = (df_cas[\"step1_CE\"] + df_cas[\"step2_CE\"] + df_cas[\"step3_CE\"]) / 3\n",
    "\n",
    "    df_cas[\"merged_peaks\"] = df_cas.apply(lambda x: merge_annotated_spectrum(merge_annotated_spectrum(x[\"sim_peaks_20\"], x[\"sim_peaks_35\"]), x[\"sim_peaks_50\"]) , axis=1)\n",
    "    df_cas[\"merged_cosine\"] = df_cas.apply(lambda x: spectral_cosine(x[\"peaks\"], x[\"merged_peaks\"]), axis=1)\n",
    "    df_cas[\"merged_sqrt_cosine\"] = df_cas.apply(lambda x: spectral_cosine(x[\"peaks\"], x[\"merged_peaks\"], transform=np.sqrt), axis=1)\n",
    "    df_cas[\"merged_sqrt_cosine_wo_prec\"] = df_cas.apply(lambda x: spectral_cosine(x[\"peaks\"], x[\"merged_peaks\"], transform=np.sqrt, remove_mz=x[\"Metabolite\"].get_theoretical_precursor_mz(x[\"Metabolite\"].metadata[\"precursor_mode\"])), axis=1)\n",
    "    df_cas[\"merged_refl_cosine\"] = df_cas.apply(lambda x: spectral_reflection_cosine(x[\"peaks\"], x[\"merged_peaks\"], transform=np.sqrt), axis=1)\n",
    "    df_cas[\"merged_steins\"] = df_cas.apply(lambda x: reweighted_dot(x[\"peaks\"], x[\"merged_peaks\"]), axis=1)\n",
    "    df_cas[\"spectral_sqrt_cosine\"] = df_cas[\"merged_sqrt_cosine\"] # just remember it is merged\n",
    "    df_cas[\"spectral_sqrt_cosine_wo_prec\"] = df_cas[\"merged_sqrt_cosine_wo_prec\"] # just remember it is merged\n",
    "\n",
    "    df_cas[\"coverage\"] = df_cas[\"Metabolite\"].apply(lambda x: x.match_stats[\"coverage\"])\n",
    "    df_cas[\"RT_pred\"] = df_cas[\"RT_pred_35\"]\n",
    "    df_cas[\"RT_dif\"] = df_cas[\"RT_dif_35\"]\n",
    "    df_cas[\"CCS_pred\"] = df_cas[\"CCS_pred_35\"]\n",
    "    df_cas[\"library\"] = \"CASMI-16\"\n",
    "    \n",
    "    if return_df:\n",
    "        return df_cas\n",
    "    \n",
    "    return df_cas[score].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raise KeyboardInterrupt(\"Stop before starting a new training run!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GNNCompiler(model_params).to(dev)\n",
    "#model.to(\"cpu\")\n",
    "# device = next(model.parameters()).device\n",
    "# device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Using pre-set train/validation keys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ynowatzk/miniforge3/envs/fiora/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidation RMSE: 0.008289\n",
      "\t >> Set new checkpoint to epoch 1\n",
      "\tValidation RMSE: 0.008082\n",
      "\t >> Set new checkpoint to epoch 2\n",
      "\tValidation RMSE: 0.007879\n",
      "\t >> Set new checkpoint to epoch 3\n",
      "\tValidation RMSE: 0.007276\n",
      "\t >> Set new checkpoint to epoch 4\n",
      "\tValidation RMSE: 0.007073\n",
      "\t >> Set new checkpoint to epoch 5\n",
      "\tValidation RMSE: 0.007071\n",
      "\t >> Set new checkpoint to epoch 6\n",
      "\tValidation RMSE: 0.006969\n",
      "\t >> Set new checkpoint to epoch 7\n",
      "\tValidation RMSE: 0.006968\n",
      "\tValidation RMSE: 0.006867\n",
      "\t >> Set new checkpoint to epoch 9\n",
      "\tValidation RMSE: 0.0067066\n",
      "\t >> Set new checkpoint to epoch 10\n",
      "\tValidation RMSE: 0.0067065\n",
      "\tValidation RMSE: 0.0066064\n",
      "\t >> Set new checkpoint to epoch 12\n",
      "\tValidation RMSE: 0.0066063\n",
      "\t >> Set new checkpoint to epoch 13\n",
      "\tValidation RMSE: 0.0065062\n",
      "\t >> Set new checkpoint to epoch 14\n",
      "\tValidation RMSE: 0.0065061\n",
      "\tValidation RMSE: 0.0065061\n",
      "\t >> Set new checkpoint to epoch 16\n",
      "\tValidation RMSE: 0.0064060\n",
      "\t >> Set new checkpoint to epoch 17\n",
      "\tValidation RMSE: 0.0065059\n",
      "\tValidation RMSE: 0.0064059\n",
      "\t >> Set new checkpoint to epoch 19\n",
      "\tValidation RMSE: 0.0065058\n",
      "\tValidation RMSE: 0.0064058\n",
      "\t >> Set new checkpoint to epoch 21\n",
      "\tValidation RMSE: 0.0064057\n",
      "\tValidation RMSE: 0.0064056\n",
      "\tValidation RMSE: 0.0064056\n",
      "\tValidation RMSE: 0.0065055\n",
      "\tValidation RMSE: 0.0063055\n",
      "\t >> Set new checkpoint to epoch 26\n",
      "\tValidation RMSE: 0.0064054\n",
      "\tValidation RMSE: 0.0064054\n",
      "\tValidation RMSE: 0.0063053\n",
      "\t >> Set new checkpoint to epoch 29\n",
      "\tValidation RMSE: 0.0064053\n",
      "\tValidation RMSE: 0.0063052\n",
      "\tValidation RMSE: 0.0063052\n",
      "\tValidation RMSE: 0.0064052\n",
      "\tValidation RMSE: 0.0063052\n",
      "\t >> Set new checkpoint to epoch 34\n",
      "\tValidation RMSE: 0.0063051\n",
      "\tValidation RMSE: 0.0063051\n",
      "\tValidation RMSE: 0.0063050\n",
      "\t >> Set new checkpoint to epoch 37\n",
      "\tValidation RMSE: 0.0063050\n",
      "\tValidation RMSE: 0.0062050\n",
      "\t >> Set new checkpoint to epoch 39\n",
      "\tValidation RMSE: 0.0063049\n",
      "\tValidation RMSE: 0.0063049\n",
      "\tValidation RMSE: 0.0062049\n",
      "\t >> Set new checkpoint to epoch 42\n",
      "\tValidation RMSE: 0.0062048\n",
      "\t >> Set new checkpoint to epoch 43\n",
      "\tValidation RMSE: 0.0063048\n",
      "\tValidation RMSE: 0.0063048\n",
      "\tValidation RMSE: 0.0063048\n",
      "\tValidation RMSE: 0.0062047\n",
      "\t >> Set new checkpoint to epoch 47\n",
      "\tValidation RMSE: 0.0062047\n",
      "\t >> Set new checkpoint to epoch 48\n",
      "\tValidation RMSE: 0.0062047\n",
      "\tValidation RMSE: 0.0063046\n",
      "\tValidation RMSE: 0.0062046\n",
      "\tValidation RMSE: 0.0063046\n",
      "\tValidation RMSE: 0.0063046\n",
      "\tValidation RMSE: 0.0062046\n",
      "\t >> Set new checkpoint to epoch 54\n",
      "\tValidation RMSE: 0.0063045\n",
      "\tValidation RMSE: 0.0062045\n",
      "\tValidation RMSE: 0.0062045\n",
      "\tValidation RMSE: 0.0061045\n",
      "\t >> Set new checkpoint to epoch 58\n",
      "\tValidation RMSE: 0.0062045\n",
      "\tValidation RMSE: 0.0062044\n",
      "\tValidation RMSE: 0.0062044\n",
      "\tValidation RMSE: 0.0062044\n",
      "\tValidation RMSE: 0.0062044\n",
      "\tValidation RMSE: 0.0062043\n",
      "\tValidation RMSE: 0.0062043\n",
      "\tValidation RMSE: 0.0062043\n",
      "\tValidation RMSE: 0.0062043\n",
      "\tValidation RMSE: 0.0061042\n",
      "\t >> Set new checkpoint to epoch 68\n",
      "\tValidation RMSE: 0.0061041\n",
      "\t >> Set new checkpoint to epoch 69\n",
      "\tValidation RMSE: 0.0061041\n",
      "\t >> Set new checkpoint to epoch 70\n",
      "\tValidation RMSE: 0.0062041\n",
      "\tValidation RMSE: 0.0061041\n",
      "\tValidation RMSE: 0.0062041\n",
      "\tValidation RMSE: 0.0061041\n",
      "\tValidation RMSE: 0.0061041\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061040\n",
      "\t >> Set new checkpoint to epoch 79\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061040\n",
      "\t >> Set new checkpoint to epoch 81\n",
      "\tValidation RMSE: 0.0061040\n",
      "\t >> Set new checkpoint to epoch 82\n",
      "\tValidation RMSE: 0.0061040\n",
      "\t >> Set new checkpoint to epoch 83\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061040\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\t >> Set new checkpoint to epoch 90\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.0061039\n",
      "\tValidation RMSE: 0.00610038\n",
      "\tValidation RMSE: 0.00610038\n",
      "\tValidation RMSE: 0.00610038\n",
      "\tValidation RMSE: 0.00610038\n",
      "\tValidation RMSE: 0.00610038\n",
      "\t >> Set new checkpoint to epoch 104\n",
      "\tValidation RMSE: 0.00610038\n",
      "\tValidation RMSE: 0.00610038\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\t >> Set new checkpoint to epoch 113\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\t >> Set new checkpoint to epoch 120\n",
      "\tValidation RMSE: 0.00600037\n",
      "\t >> Set new checkpoint to epoch 121\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00600037\n",
      "\t >> Set new checkpoint to epoch 124\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00600037\n",
      "\t >> Set new checkpoint to epoch 128\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00610037\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\t >> Set new checkpoint to epoch 138\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00610036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\t >> Set new checkpoint to epoch 150\n",
      "\tValidation RMSE: 0.00600036\n",
      "\t >> Set new checkpoint to epoch 151\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\t >> Set new checkpoint to epoch 156\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600036\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "\tValidation RMSE: 0.00600035\n",
      "Finished Training!\n",
      "{'epoch': 156, 'val_loss': 3.6315686884336174e-05, 'file': '../../checkpoint_training.best.pt'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training model\")\n",
    "model, checkpoints = train_new_model() # continue_with_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 156, 'val_loss': 3.6315686884336174e-05, 'file': '../../checkpoint_training.best.pt'}\n",
      "0.006026249819277008\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "print(checkpoints) \n",
    "print(np.sqrt(checkpoints[\"val_loss\"]))\n",
    "model_at_last_epoch = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1780528/3303304107.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
      "/tmp/ipykernel_1780528/3303304107.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
      "/tmp/ipykernel_1780528/3303304107.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
      "/tmp/ipykernel_1780528/3303304107.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
      "/tmp/ipykernel_1780528/3303304107.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
      "/tmp/ipykernel_1780528/3303304107.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
      "/tmp/ipykernel_1780528/3303304107.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
      "/tmp/ipykernel_1780528/3303304107.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n"
     ]
    }
   ],
   "source": [
    "model = GNNCompiler.load(checkpoints[\"file\"]).to(dev)\n",
    "score = \"spectral_sqrt_cosine\"\n",
    "\n",
    "val_results = test_model(model, df_train[df_train[\"dataset\"]== \"validation\"], score=score)\n",
    "test_results = test_model(model, df_test, score=score)\n",
    "casmi16_results = test_cas16(model, score=score)\n",
    "casmi16_p = test_cas16(model, df_cas[df_cas[\"Precursor_type\"] == \"[M+H]+\"], score=score)\n",
    "casmi16_n = test_cas16(model, df_cas[df_cas[\"Precursor_type\"] == \"[M-H]-\"], score=score)\n",
    "casmi22_results = test_model(model, df_cas22, score=score)\n",
    "casmi22_p = test_model(model, df_cas22[df_cas22[\"Precursor_type\"] == \"[M+H]+\"], score=score)\n",
    "casmi22_n = test_model(model, df_cas22[df_cas22[\"Precursor_type\"] == \"[M-H]-\"], score=score)\n",
    "    \n",
    "results = [{\"model\": model, \"validation\": val_results, \"test\": test_results, \"casmi16\": casmi16_results, \"casmi22\": casmi22_results, \"casmi16+\": casmi16_p, \"casmi16-\": casmi16_n, \"casmi22+\": casmi22_p, \"casmi22-\": casmi22_n}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1780528/3303304107.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
      "/tmp/ipykernel_1780528/3303304107.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
      "/tmp/ipykernel_1780528/3303304107.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
      "/tmp/ipykernel_1780528/3303304107.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n",
      "/tmp/ipykernel_1780528/3303304107.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"NCE\"] = 20.0 # actually stepped NCE 20/35/50\n",
      "/tmp/ipykernel_1780528/3303304107.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"CE\"] = df_cas[[\"NCE\", \"PRECURSOR_MZ\"]].apply(lambda x: NCE_to_eV(x[\"NCE\"], x[\"PRECURSOR_MZ\"]), axis=1)\n",
      "/tmp/ipykernel_1780528/3303304107.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"step1_CE\"] = df_cas[\"CE\"]\n",
      "/tmp/ipykernel_1780528/3303304107.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cas[\"summary\"] = df_cas.apply(lambda x: {key: x[name] for key, name in metadata_key_map16.items()}, axis=1)\n"
     ]
    }
   ],
   "source": [
    "score = \"spectral_sqrt_cosine_wo_prec\"\n",
    "val_results = test_model(model, df_train[df_train[\"dataset\"]== \"validation\"], score=score)\n",
    "test_results = test_model(model, df_test, score=score)\n",
    "casmi16_results = test_cas16(model, score=score)\n",
    "casmi16_p = test_cas16(model, df_cas[df_cas[\"Precursor_type\"] == \"[M+H]+\"], score=score)\n",
    "casmi16_n = test_cas16(model, df_cas[df_cas[\"Precursor_type\"] == \"[M-H]-\"], score=score)\n",
    "casmi22_results = test_model(model, df_cas22, score=score)\n",
    "casmi22_p = test_model(model, df_cas22[df_cas22[\"Precursor_type\"] == \"[M+H]+\"], score=score)\n",
    "casmi22_n = test_model(model, df_cas22[df_cas22[\"Precursor_type\"] == \"[M-H]-\"], score=score)\n",
    "    \n",
    "results_wop = [{\"model\": model, \"validation\": val_results, \"test\": test_results, \"casmi16\": casmi16_results, \"casmi22\": casmi22_results, \"casmi16+\": casmi16_p, \"casmi16-\": casmi16_n, \"casmi22+\": casmi22_p, \"casmi22-\": casmi22_n}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>casmi16</th>\n",
       "      <th>casmi22</th>\n",
       "      <th>casmi16+</th>\n",
       "      <th>casmi16-</th>\n",
       "      <th>casmi22+</th>\n",
       "      <th>casmi22-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GNNCompiler(\\n  (node_embedding): FeatureEmbed...</td>\n",
       "      <td>0.789238</td>\n",
       "      <td>0.738194</td>\n",
       "      <td>0.677314</td>\n",
       "      <td>0.25998</td>\n",
       "      <td>0.658437</td>\n",
       "      <td>0.730632</td>\n",
       "      <td>0.253072</td>\n",
       "      <td>0.265494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  validation      test  \\\n",
       "0  GNNCompiler(\\n  (node_embedding): FeatureEmbed...    0.789238  0.738194   \n",
       "\n",
       "    casmi16  casmi22  casmi16+  casmi16-  casmi22+  casmi22-  \n",
       "0  0.677314  0.25998  0.658437  0.730632  0.253072  0.265494  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG = pd.DataFrame(results)\n",
    "eval_columns = LOG.columns[1:].fillna(0.0)\n",
    "LOG[eval_columns] = LOG[eval_columns].apply(lambda x: x.apply(np.median))\n",
    "\n",
    "LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>casmi16</th>\n",
       "      <th>casmi22</th>\n",
       "      <th>casmi16+</th>\n",
       "      <th>casmi16-</th>\n",
       "      <th>casmi22+</th>\n",
       "      <th>casmi22-</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GNNCompiler(\\n  (node_embedding): FeatureEmbed...</td>\n",
       "      <td>0.755023</td>\n",
       "      <td>0.691917</td>\n",
       "      <td>0.380801</td>\n",
       "      <td>0.230422</td>\n",
       "      <td>0.364927</td>\n",
       "      <td>0.426074</td>\n",
       "      <td>0.217827</td>\n",
       "      <td>0.262897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model  validation      test  \\\n",
       "0  GNNCompiler(\\n  (node_embedding): FeatureEmbed...    0.755023  0.691917   \n",
       "\n",
       "    casmi16   casmi22  casmi16+  casmi16-  casmi22+  casmi22-  \n",
       "0  0.380801  0.230422  0.364927  0.426074  0.217827  0.262897  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOG = pd.DataFrame(results_wop).fillna(0.0)\n",
    "eval_columns = LOG.columns[1:]\n",
    "LOG[eval_columns] = LOG[eval_columns].apply(lambda x: x.apply(np.nan_to_num).apply(np.median))\n",
    "\n",
    "LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'param_tag': 'default',\n",
       " 'gnn_type': 'RGCNConv',\n",
       " 'depth': 6,\n",
       " 'hidden_dimension': 300,\n",
       " 'dense_layers': 2,\n",
       " 'embedding_aggregation': 'concat',\n",
       " 'embedding_dimension': 300,\n",
       " 'input_dropout': 0.2,\n",
       " 'latent_dropout': 0.1,\n",
       " 'node_feature_layout': {'symbol': 10, 'num_hydrogen': 5, 'ring_type': 5},\n",
       " 'edge_feature_layout': {'bond_type': 4, 'ring_type': 5},\n",
       " 'static_feature_dimension': 8,\n",
       " 'static_rt_feature_dimension': 7,\n",
       " 'output_dimension': 10,\n",
       " 'atom_features': ['bond_type', 'ring_type'],\n",
       " 'setup_features': ['collision_energy',\n",
       "  'molecular_weight',\n",
       "  'precursor_mode',\n",
       "  'instrument'],\n",
       " 'setup_features_categorical_set': {'instrument': ['HCD'],\n",
       "  'precursor_mode': ['[M+H]+', '[M-H]-', '[M]+', '[M]-']},\n",
       " 'rt_features': ['molecular_weight', 'precursor_mode', 'instrument'],\n",
       " 'rt_supported': False,\n",
       " 'ccs_supported': False,\n",
       " 'version': 'x.x.x',\n",
       " 'training_label': 'compiled_probsSQRT'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOG = pd.DataFrame(results)\n",
    "# eval_columns = LOG.columns[3:]\n",
    "\n",
    "# home_path = f\"{home}/data/metabolites/benchmarking/\"\n",
    "# NAME = model_params[\"gnn_type\"] + \"_depth.csv\"\n",
    "# for col in eval_columns:\n",
    "#     LOG[col] = LOG[col].apply(lambda x: str(list(x)))\n",
    "# LOG.to_csv(home_path + NAME, index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGIC = pd.read_csv(home_path + NAME, sep=\"\\t\")\n",
    "# for col in eval_columns:\n",
    "#     LOGIC[col] = LOGIC[col].apply(lambda x: ast.literal_eval(x.replace('nan', 'None')))\n",
    "\n",
    "# LOGIC[eval_columns] = LOGIC[eval_columns].apply(lambda x: x.apply(np.median))\n",
    "# LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Halt! Make sure you wish to save/overwrite model files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHalt! Make sure you wish to save/overwrite model files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Halt! Make sure you wish to save/overwrite model files"
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt(\"Halt! Make sure you wish to save/overwrite model files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/ynowatzk/data/metabolites/pretrained_models/v0.1.1_OS_depth6_April25_v2.pt\n"
     ]
    }
   ],
   "source": [
    "depth = model_params[\"depth\"]\n",
    "MODEL_PATH = f\"{home}/data/metabolites/pretrained_models/v0.1.1_OS_depth{depth}_April25_vXXX.pt\"\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"Saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiora.GNN.GNNModules import GNNCompiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[32, 3], edge_index=[2, 72], edge_attr=[72, 2], y=[72, 1], edge_type=[72], static_graph_features=[1, 8], static_edge_features=[72, 8], static_rt_features=[1, 7], compiled_probs=[74], compiled_probs2=[146], compiled_probs6=[434], compiled_probsALL=[722], compiled_probsSQRT=[722], compiled_probsDEPRE=[722], compiled_probs_wo_prec=[722], compiled_counts=[74], edge_break_count=[72, 1], retention_time=[1, 1], retention_mask=[1, 1], ccs=[1, 1], ccs_mask=[1, 1], validation_mask=[72, 1], compiled_validation_mask=[74], compiled_validation_mask2=[146], compiled_validation_mask6=[434], compiled_validation_maskALL=[722], group_id=11, weight=[1, 1], weight_tensor=[722], is_node_aromatic=[32, 1], is_edge_aromatic=[72, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"Metabolite\"].iloc[0].as_geometric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=\"cuda:0\"\n",
    "mymy = GNNCompiler.load(MODEL_PATH) # f\"{home}/data/metabolites/pretrained_models/v0.0.1_merged_2.pt\"\n",
    "#mymy.load_state_dict(torch.load(f\"{home}/data/metabolites/pretrained_models/test.pt\"))\n",
    "mymy.eval()\n",
    "mymy = mymy.to(dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6777992132153692"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(test_model(mymy, df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2357750642225984"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(test_model(mymy, df_cas22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(MODEL_PATH.replace(\".pt\", \"_params.json\"), 'r') as fp:\n",
    "    p = json.load(fp)\n",
    "hh = GNNCompiler(p)\n",
    "hh.load_state_dict(torch.load(MODEL_PATH.replace(\".pt\", \"_state.pt\")))\n",
    "hh.eval()\n",
    "hh = hh.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2357750642225984"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(test_model(hh, df_cas22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "TODO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ynowatzk/repos/metabolites/notebooks/train_model.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsv2111/home/ynowatzk/repos/metabolites/notebooks/train_model.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTODO\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: TODO"
     ]
    }
   ],
   "source": [
    "raise KeyboardInterrupt(\"TODO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       group_id                                             SMILES  \\\n",
      "91358       695  CCCCCCCCCCCCCCCC(=O)OC[C@@H](O)COP(=O)([O-])OC...   \n",
      "91888      1028  CN1c2c([nH]c(N)nc2=O)NCC1CNc1ccc(C(=O)N[C@@H](...   \n",
      "91898      1030  CC(C)C[C@@H](N)[C@H](O)C(=O)N[C@H](C(=O)N[C@H]...   \n",
      "92031      8617        CCCCC[C@H](O)/C=C/C=C\\C/C=C\\C/C=C\\CCCC(=O)O   \n",
      "92174      1061                    CC(=O)N[C@@H](CCCNC(=N)N)C(=O)O   \n",
      "\n",
      "      Precursor_type  \n",
      "91358         [M-H]-  \n",
      "91888         [M-H]-  \n",
      "91898         [M-H]-  \n",
      "92031         [M-H]-  \n",
      "92174         [M-H]-  \n"
     ]
    }
   ],
   "source": [
    "## prepare output for for CFM-ID\n",
    "import os\n",
    "save_df = False\n",
    "cfm_directory = f\"{home}/data/metabolites/cfm-id/\"\n",
    "name = \"test_split_negative_solutions_cfm.txt\"\n",
    "df_cfm = df_test[[\"group_id\", \"SMILES\", \"Precursor_type\"]]\n",
    "df_n = df_cfm[df_cfm[\"Precursor_type\"] == \"[M-H]-\"].drop_duplicates(subset='group_id', keep='first')\n",
    "df_p = df_cfm[df_cfm[\"Precursor_type\"] == \"[M+H]+\"].drop_duplicates(subset='group_id', keep='first')\n",
    "\n",
    "print(df_n.head())\n",
    "\n",
    "if save_df:\n",
    "    file = os.path.join(cfm_directory, name)\n",
    "    df_n[[\"group_id\", \"SMILES\"]].to_csv(file, index=False, header=False, sep=\" \")\n",
    "    \n",
    "    name = name.replace(\"negative\", \"positive\")\n",
    "    file = os.path.join(cfm_directory, name)\n",
    "    df_p[[\"group_id\", \"SMILES\"]].to_csv(file, index=False, header=False, sep=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
