{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train CCS Predicting Models, Predict CCS Values of Metabolites, Compare Them</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import Data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot fonts globally\n",
    "\n",
    "mpl.rcParams['font.size'] = 14\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 16   # Title font size\n",
    "mpl.rcParams['axes.labelsize'] = 14   # Axis label font size\n",
    "mpl.rcParams['xtick.labelsize'] = 10  # X-axis tick label font size\n",
    "mpl.rcParams['ytick.labelsize'] = 10  # Y-axis tick label font size\n",
    "mpl.rcParams['legend.fontsize'] = 11  # Legend font size\n",
    "mpl.rcParams['figure.titlesize'] = 18 # Figure title font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Molecule Name</th>\n",
       "      <th>Molecular Formula</th>\n",
       "      <th>METLIN ID</th>\n",
       "      <th>Precursor Adduct</th>\n",
       "      <th>CCS1</th>\n",
       "      <th>CCS2</th>\n",
       "      <th>CCS3</th>\n",
       "      <th>CCS_AVG</th>\n",
       "      <th>% CV</th>\n",
       "      <th>m/z</th>\n",
       "      <th>...</th>\n",
       "      <th>m/z.1</th>\n",
       "      <th>Dimer</th>\n",
       "      <th>Dimer.1</th>\n",
       "      <th>dimer line</th>\n",
       "      <th>CCS</th>\n",
       "      <th>m/z.2</th>\n",
       "      <th>pubChem</th>\n",
       "      <th>inchi</th>\n",
       "      <th>smiles</th>\n",
       "      <th>InChIKEY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-[3-(2,3-dihydroindol-1-yl)propyl]-1-[(4-fluo...</td>\n",
       "      <td>C19H22FN3O</td>\n",
       "      <td>1181481.0</td>\n",
       "      <td>328.1820[M+H]</td>\n",
       "      <td>176.63</td>\n",
       "      <td>176.63</td>\n",
       "      <td>176.63</td>\n",
       "      <td>176.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>328.1820</td>\n",
       "      <td>...</td>\n",
       "      <td>328.1820</td>\n",
       "      <td>209.886594</td>\n",
       "      <td>Monomer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25345055</td>\n",
       "      <td>InChI=1S/C19H22FN3O/c20-17-8-6-15(7-9-17)14-22...</td>\n",
       "      <td>O=C(NCc1ccc(cc1)F)NCCCN1CCc2c1cccc2</td>\n",
       "      <td>DWYWDNMZIWKHFM-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3-{[1,1'-biphenyl]-4-ylmethylidene}-6-fluoro-2...</td>\n",
       "      <td>C22H15FO3S</td>\n",
       "      <td>1191359.0</td>\n",
       "      <td>379.0799[M+H]</td>\n",
       "      <td>192.26</td>\n",
       "      <td>192.26</td>\n",
       "      <td>192.26</td>\n",
       "      <td>192.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>379.0799</td>\n",
       "      <td>...</td>\n",
       "      <td>379.0799</td>\n",
       "      <td>223.588309</td>\n",
       "      <td>Monomer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>310.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>InChI=1S/C22H15FO3S/c23-19-10-11-21-20(13-19)2...</td>\n",
       "      <td>Fc1ccc2c(c1)C(=O)C(=Cc1ccc(cc1)c1ccccc1)CS2(=O)=O</td>\n",
       "      <td>MFNGEIZTLYGVQK-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-{[3-(4-methoxyphenyl)-1,2,4-oxadiazol-5-yl]m...</td>\n",
       "      <td>C25H28N4O3</td>\n",
       "      <td>1228206.0</td>\n",
       "      <td>433.2234[M+H]</td>\n",
       "      <td>211.12</td>\n",
       "      <td>211.12</td>\n",
       "      <td>211.12</td>\n",
       "      <td>211.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>433.2234</td>\n",
       "      <td>...</td>\n",
       "      <td>433.2234</td>\n",
       "      <td>238.163739</td>\n",
       "      <td>Monomer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17541371</td>\n",
       "      <td>InChI=1S/C25H28N4O3/c1-31-21-10-8-19(9-11-21)2...</td>\n",
       "      <td>COc1ccc(cc1)c1noc(n1)CN1CCN(CC1)C(=O)C1(CCC1)c...</td>\n",
       "      <td>YPHGAJRHIVSPSX-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-[2-oxo-2-(4-phenylmethanesulfonylpiperazin-1...</td>\n",
       "      <td>C22H30N4O5S</td>\n",
       "      <td>1176932.0</td>\n",
       "      <td>463.2010[M+H]</td>\n",
       "      <td>204.22</td>\n",
       "      <td>204.22</td>\n",
       "      <td>204.22</td>\n",
       "      <td>204.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>463.2010</td>\n",
       "      <td>...</td>\n",
       "      <td>463.2010</td>\n",
       "      <td>246.233709</td>\n",
       "      <td>Monomer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16295966</td>\n",
       "      <td>InChI=1S/C22H30N4O5S/c27-19(16-26-20(28)22(23-...</td>\n",
       "      <td>O=C(N1CCN(CC1)S(=O)(=O)Cc1ccccc1)CN1C(=O)NC2(C...</td>\n",
       "      <td>ALHVXFHLDXYEII-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-{[1,1'-biphenyl]-2-ylamino}-1-(morpholin-4-y...</td>\n",
       "      <td>C18H20N2O2</td>\n",
       "      <td>1183857.0</td>\n",
       "      <td>297.1598[M+H]</td>\n",
       "      <td>174.47</td>\n",
       "      <td>174.47</td>\n",
       "      <td>174.47</td>\n",
       "      <td>174.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>297.1598</td>\n",
       "      <td>...</td>\n",
       "      <td>297.1598</td>\n",
       "      <td>201.535418</td>\n",
       "      <td>Monomer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17463671</td>\n",
       "      <td>InChI=1S/C18H20N2O2/c21-18(20-10-12-22-13-11-2...</td>\n",
       "      <td>O=C(N1CCOCC1)CNc1ccccc1c1ccccc1</td>\n",
       "      <td>YWINZXNEVGTFAX-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65283</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>XKTJEHXBSIDWRH-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65284</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>XKTJEHXBSIDWRH-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65285</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>XKTJEHXBSIDWRH-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65286</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>XKTJEHXBSIDWRH-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65287</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>--</td>\n",
       "      <td>XKTJEHXBSIDWRH-UHFFFAOYSA-N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65288 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Molecule Name Molecular Formula  \\\n",
       "0      3-[3-(2,3-dihydroindol-1-yl)propyl]-1-[(4-fluo...        C19H22FN3O   \n",
       "1      3-{[1,1'-biphenyl]-4-ylmethylidene}-6-fluoro-2...        C22H15FO3S   \n",
       "2      1-{[3-(4-methoxyphenyl)-1,2,4-oxadiazol-5-yl]m...        C25H28N4O3   \n",
       "3      3-[2-oxo-2-(4-phenylmethanesulfonylpiperazin-1...       C22H30N4O5S   \n",
       "4      2-{[1,1'-biphenyl]-2-ylamino}-1-(morpholin-4-y...        C18H20N2O2   \n",
       "...                                                  ...               ...   \n",
       "65283                                                NaN               NaN   \n",
       "65284                                                NaN               NaN   \n",
       "65285                                                NaN               NaN   \n",
       "65286                                                NaN               NaN   \n",
       "65287                                                NaN               NaN   \n",
       "\n",
       "       METLIN ID Precursor Adduct    CCS1    CCS2    CCS3  CCS_AVG  % CV  \\\n",
       "0      1181481.0    328.1820[M+H]  176.63  176.63  176.63   176.63   0.0   \n",
       "1      1191359.0    379.0799[M+H]  192.26  192.26  192.26   192.26   0.0   \n",
       "2      1228206.0    433.2234[M+H]  211.12  211.12  211.12   211.12   0.0   \n",
       "3      1176932.0    463.2010[M+H]  204.22  204.22  204.22   204.22   0.0   \n",
       "4      1183857.0    297.1598[M+H]  174.47  174.47  174.47   174.47   0.0   \n",
       "...          ...              ...     ...     ...     ...      ...   ...   \n",
       "65283        NaN              NaN     NaN     NaN     NaN      NaN   NaN   \n",
       "65284        NaN              NaN     NaN     NaN     NaN      NaN   NaN   \n",
       "65285        NaN              NaN     NaN     NaN     NaN      NaN   NaN   \n",
       "65286        NaN              NaN     NaN     NaN     NaN      NaN   NaN   \n",
       "65287        NaN              NaN     NaN     NaN     NaN      NaN   NaN   \n",
       "\n",
       "            m/z  ...     m/z.1       Dimer  Dimer.1 dimer line    CCS  m/z.2  \\\n",
       "0      328.1820  ...  328.1820  209.886594  Monomer        NaN  135.0   50.0   \n",
       "1      379.0799  ...  379.0799  223.588309  Monomer        NaN  310.0  700.0   \n",
       "2      433.2234  ...  433.2234  238.163739  Monomer        NaN    NaN    NaN   \n",
       "3      463.2010  ...  463.2010  246.233709  Monomer        NaN    NaN    NaN   \n",
       "4      297.1598  ...  297.1598  201.535418  Monomer        NaN    NaN    NaN   \n",
       "...         ...  ...       ...         ...      ...        ...    ...    ...   \n",
       "65283       NaN  ...       NaN         NaN      NaN        NaN    NaN    NaN   \n",
       "65284       NaN  ...       NaN         NaN     3825        NaN    NaN    NaN   \n",
       "65285       NaN  ...       NaN         NaN    61457        NaN    NaN    NaN   \n",
       "65286       NaN  ...       NaN         NaN      NaN        NaN    NaN    NaN   \n",
       "65287       NaN  ...       NaN         NaN    65282        NaN    NaN    NaN   \n",
       "\n",
       "        pubChem                                              inchi  \\\n",
       "0      25345055  InChI=1S/C19H22FN3O/c20-17-8-6-15(7-9-17)14-22...   \n",
       "1           NaN  InChI=1S/C22H15FO3S/c23-19-10-11-21-20(13-19)2...   \n",
       "2      17541371  InChI=1S/C25H28N4O3/c1-31-21-10-8-19(9-11-21)2...   \n",
       "3      16295966  InChI=1S/C22H30N4O5S/c27-19(16-26-20(28)22(23-...   \n",
       "4      17463671  InChI=1S/C18H20N2O2/c21-18(20-10-12-22-13-11-2...   \n",
       "...         ...                                                ...   \n",
       "65283        --                                                 --   \n",
       "65284        --                                                 --   \n",
       "65285        --                                                 --   \n",
       "65286        --                                                 --   \n",
       "65287        --                                                 --   \n",
       "\n",
       "                                                  smiles  \\\n",
       "0                    O=C(NCc1ccc(cc1)F)NCCCN1CCc2c1cccc2   \n",
       "1      Fc1ccc2c(c1)C(=O)C(=Cc1ccc(cc1)c1ccccc1)CS2(=O)=O   \n",
       "2      COc1ccc(cc1)c1noc(n1)CN1CCN(CC1)C(=O)C1(CCC1)c...   \n",
       "3      O=C(N1CCN(CC1)S(=O)(=O)Cc1ccccc1)CN1C(=O)NC2(C...   \n",
       "4                        O=C(N1CCOCC1)CNc1ccccc1c1ccccc1   \n",
       "...                                                  ...   \n",
       "65283                                                 --   \n",
       "65284                                                 --   \n",
       "65285                                                 --   \n",
       "65286                                                 --   \n",
       "65287                                                 --   \n",
       "\n",
       "                          InChIKEY  \n",
       "0      DWYWDNMZIWKHFM-UHFFFAOYSA-N  \n",
       "1      MFNGEIZTLYGVQK-UHFFFAOYSA-N  \n",
       "2      YPHGAJRHIVSPSX-UHFFFAOYSA-N  \n",
       "3      ALHVXFHLDXYEII-UHFFFAOYSA-N  \n",
       "4      YWINZXNEVGTFAX-UHFFFAOYSA-N  \n",
       "...                            ...  \n",
       "65283  XKTJEHXBSIDWRH-UHFFFAOYSA-N  \n",
       "65284  XKTJEHXBSIDWRH-UHFFFAOYSA-N  \n",
       "65285  XKTJEHXBSIDWRH-UHFFFAOYSA-N  \n",
       "65286  XKTJEHXBSIDWRH-UHFFFAOYSA-N  \n",
       "65287  XKTJEHXBSIDWRH-UHFFFAOYSA-N  \n",
       "\n",
       "[65288 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccs_df = pd.read_csv('/home/lbarbut/data/ccs/METLIN_IMS_all plot CV %3C2% _dimers.csv')\n",
    "ccs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = True\n",
    "if debug_mode:\n",
    "    # Shuffle, because all first 1000 rows have precursor [M+H]+\n",
    "    ccs_df = ccs_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Keep only the first 1000 rows\n",
    "    ccs_df = ccs_df.iloc[:1000]\n",
    "\n",
    "    # Reset the index (optional)\n",
    "    ccs_df = ccs_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Data Preprocessing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN Precusor Adduct rows\n",
    "ccs_df = ccs_df.dropna(subset=['Precursor Adduct'])\n",
    "\n",
    "# remove the m/z number before the Precursor Adduct since the info is also in m/z column\n",
    "ccs_df.loc[:, 'Precursor Adduct'] = ccs_df['Precursor Adduct'].str.replace(r'[0-9.]+', '', regex=True)\n",
    "\n",
    "# remove [M+Na] rows\n",
    "ccs_df = ccs_df[ccs_df['Precursor Adduct'] != '[M+Na]']\n",
    "\n",
    "# change [M+H] to [M+H]+ and [M-H] to [M-H]-\n",
    "ccs_df[\"Precursor Adduct\"] = ccs_df[\"Precursor Adduct\"].replace({\n",
    "    \"[M+H]\": \"[M+H]+\",\n",
    "    \"[M-H]\": \"[M-H]-\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precursor Adduct\n",
      "[M+H]+    28136\n",
      "[M-H]-    19697\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ccs_df['Precursor Adduct'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_smiles(smiles):\n",
    "    try:\n",
    "        return Chem.MolFromSmiles(smiles) is not None  # True if valid, False if invalid\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Apply validation\n",
    "ccs_df[\"smiles_is_valid\"] = ccs_df[\"smiles\"].apply(is_valid_smiles)\n",
    "\n",
    "invalid_smiles = ccs_df.loc[~ccs_df[\"smiles_is_valid\"], \"smiles\"]  # Select invalid SMILES\n",
    "# print(\"Invalid SMILES strings:\")\n",
    "# print(invalid_smiles.tolist())\n",
    "\n",
    "# Filter out invalid SMILES, afterwards drop the is_valid column\n",
    "ccs_df = ccs_df[ccs_df[\"smiles_is_valid\"]].drop(columns=[\"smiles_is_valid\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN CCS_AVG rows\n",
    "ccs_df = ccs_df.dropna(subset=['CCS_AVG'])\n",
    "ccs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df[\"Instrument_type\"] = \"timsTOF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df['CE'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Use Fiora To Predict CCS Values of Metabolites</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Create Metabolite column by Creating Molecular Graphs of Metabolites\n",
    "\n",
    "from fiora.MOL.Metabolite import Metabolite\n",
    "from fiora.GNN.AtomFeatureEncoder import AtomFeatureEncoder\n",
    "from fiora.GNN.BondFeatureEncoder import BondFeatureEncoder\n",
    "from fiora.GNN.SetupFeatureEncoder import SetupFeatureEncoder\n",
    "\n",
    "CE_upper_limit = 100.0\n",
    "weight_upper_limit = 1000.0\n",
    "\n",
    "ccs_df[\"Metabolite\"] = ccs_df[\"smiles\"].apply(Metabolite)\n",
    "ccs_df[\"Metabolite\"].apply(lambda x: x.create_molecular_structure_graph())\n",
    "\n",
    "node_encoder = AtomFeatureEncoder(feature_list=[\"symbol\", \"num_hydrogen\", \"ring_type\"])\n",
    "bond_encoder = BondFeatureEncoder(feature_list=[\"bond_type\", \"ring_type\"])\n",
    "setup_encoder = SetupFeatureEncoder(feature_list=[\"collision_energy\", \"molecular_weight\", \"precursor_mode\", \"instrument\"])\n",
    "rt_encoder = SetupFeatureEncoder(feature_list=[\"molecular_weight\", \"precursor_mode\", \"instrument\"])\n",
    "\n",
    "setup_encoder.normalize_features[\"collision_energy\"][\"max\"] = CE_upper_limit \n",
    "setup_encoder.normalize_features[\"molecular_weight\"][\"max\"] = weight_upper_limit \n",
    "rt_encoder.normalize_features[\"molecular_weight\"][\"max\"] = weight_upper_limit \n",
    "\n",
    "ccs_df[\"Metabolite\"].apply(lambda x: x.compute_graph_attributes(node_encoder, bond_encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique Metabolite identifiers\n",
    "print(\"Unique smiles: \" + str(len(ccs_df[\"smiles\"].unique())))\n",
    "\n",
    "metabolite_id_map = {}\n",
    "for metabolite in ccs_df[\"Metabolite\"]:\n",
    "    is_new = True\n",
    "    for id, other in metabolite_id_map.items():\n",
    "        if metabolite == other:\n",
    "            metabolite.set_id(id)\n",
    "            is_new = False\n",
    "            break\n",
    "    if is_new:\n",
    "        new_id = len(metabolite_id_map)\n",
    "        metabolite.id = new_id\n",
    "        metabolite_id_map[new_id] = metabolite\n",
    "\n",
    "ccs_df[\"group_id\"] = ccs_df[\"Metabolite\"].apply(lambda x: x.get_id())\n",
    "ccs_df[\"num_per_group\"] = ccs_df[\"group_id\"].map(ccs_df[\"group_id\"].value_counts())\n",
    "\n",
    "for i, data in ccs_df.iterrows():\n",
    "    data[\"Metabolite\"].set_loss_weight(1.0 / data[\"num_per_group\"])\n",
    "ccs_df[\"loss_weight\"] = ccs_df[\"Metabolite\"].apply(lambda x: x.loss_weight)\n",
    "\n",
    "def print_df_stats(df):\n",
    "    num_spectra = df.shape[0]\n",
    "    num_ids = len(df[\"group_id\"].unique())\n",
    "    \n",
    "    print(f\"Dataframe stats: {num_spectra} spectra covering {num_ids} unique structures\")\n",
    "\n",
    "print_df_stats(ccs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df.apply(lambda x: x[\"Metabolite\"].set_id(x[\"group_id\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of CCS_AVG column\n",
    "\n",
    "plt.hist(ccs_df['CCS_AVG'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Average CCS Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Average CCS Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Predict CCS Values</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "from fiora.GNN.GNNModules import GNNCompiler\n",
    "from fiora.MS.SimulationFramework import SimulationFramework\n",
    "from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper class to add trainable LoRA-like deltas to a frozen model\n",
    "class FinetuneFioraWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.model = base_model\n",
    "\n",
    "        # Freeze base model parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Create trainable deltas (same shape as each base parameter)\n",
    "        self.deltas = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros_like(p, requires_grad=True))\n",
    "            for p in self.model.parameters()\n",
    "        ])\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        \"\"\"Override to return only the trainable deltas\"\"\"\n",
    "        return self.deltas\n",
    "\n",
    "    def named_parameters(self, prefix: str = '', recurse: bool = True):\n",
    "        \"\"\"Optional override for named parameters (for optimizers, logs)\"\"\"\n",
    "        names = [name for name, _ in self.model.named_parameters()]\n",
    "        return [(prefix + name, delta) for name, delta in zip(names, self.deltas)]\n",
    "\n",
    "    @contextmanager\n",
    "    def apply_deltas(self):\n",
    "        \"\"\"\n",
    "        Context manager: adds deltas to original weights before forward pass,\n",
    "        and restores originals afterward.\n",
    "        \"\"\"\n",
    "        originals = [p.detach().clone() for p in self.model.parameters()]\n",
    "        for p, d in zip(self.model.parameters(), self.deltas):\n",
    "            p.data.add_(d)\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            with torch.no_grad():\n",
    "                for p, o in zip(self.model.parameters(), originals):\n",
    "                    p.data.copy_(o)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with self.apply_deltas():\n",
    "            return self.model(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "\n",
    "dev=\"cuda:0\"\n",
    "#MODEL_PATH = f\"{home}/data/metabolites/pretrained_models/pre_package/v0.0.1_merged_depth6_Jan24.pt\"\n",
    "MODEL_PATH = \"/home/lbarbut/models/ccs_models/v0.0.1_merged_depth6_Aug24_sqrt+CCS+RT_drop3.pt\" # New sqrt model (improved)\n",
    "\n",
    "try:\n",
    "    base_model= GNNCompiler.load_from_state_dict(MODEL_PATH)\n",
    "    print(\"Model loaded from state dict without errors.\")\n",
    "except:\n",
    "    raise NameError(\"Error: Failed loading from state dict.\")\n",
    "        \n",
    "\n",
    "base_model.eval()\n",
    "base_model = base_model.to(dev)\n",
    "\n",
    "base_model.model_params\n",
    "\n",
    "spectral_modules = [\"node_embedding\", \"edge_embedding\", \"GNN_module\", \"edge_module\", \"precursor_module\", \"RT_module\"]\n",
    "# for module in spectral_modules:\n",
    "#     model.freeze_submodule(module)\n",
    "\n",
    "for name, param in base_model.named_parameters():\n",
    "    if param.requires_grad:print(f\"{name}: requires gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset and randomize model weights\n",
    "\n",
    "def reset_weights_and_xavier_init(model: torch.nn.Module):\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'reset_parameters'):\n",
    "            try:\n",
    "                module.reset_parameters()\n",
    "                print(f\"Reset: {name} ({module.__class__.__name__})\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to reset: {name} ({module.__class__.__name__}) — {e}\")\n",
    "\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            print(f\"Xavier initialized: {name}.weight\")\n",
    "            if module.bias is not None:\n",
    "                init.zeros_(module.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_weights_and_xavier_init(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetuneFioraWrapper(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model content\n",
    "\n",
    "# print(\"Check model content:\")\n",
    "# print(model)\n",
    "# print(\"=========================\")\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name, \"->\", module.__class__.__name__)\n",
    "# print(\"=========================\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name} | shape: {param.shape} | requires_grad: {param.requires_grad}\")\n",
    "\n",
    "# print(\"Checked Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in base_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: requires gradients\")\n",
    "    # else:\n",
    "    #     print(f\"{name}: does not require gradients (frozen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Include Metadata</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rings(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)  # Convert SMILES to RDKit molecule\n",
    "    if mol is None:\n",
    "        return 0\n",
    "    return len(Chem.GetSSSR(mol))  # Get ring count\n",
    "\n",
    "ccs_df[\"ring_count\"] = ccs_df[\"smiles\"].apply(count_rings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fiora.MOL.constants\n",
    "\n",
    "# Do rare elements (usually heavier) cause worse predictions?\n",
    "\n",
    "def rare_element_included_smiles(smiles):\n",
    "    rare_elements = fiora.MOL.constants.RARE_ELEMENTS\n",
    "    \n",
    "    return any(re.search(rf'\\b{elem}\\b', smiles) for elem in rare_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique elements across all metabolites in the DataFrame\n",
    "\n",
    "metabolite_series = ccs_df['Metabolite']\n",
    "all_unique_elements_set = set()\n",
    "for metabolite in metabolite_series:\n",
    "    elements = metabolite.node_elements\n",
    "    all_unique_elements_set.update(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df['rare_element_included'] = ccs_df[\"smiles\"].apply(lambda x: rare_element_included_smiles(x))\n",
    "ccs_df['abs_elem_distr_vec'] = ccs_df[\"Metabolite\"].apply(lambda x: Metabolite.calc_abs_elem_distr_vec(x, all_unique_elements_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_key = { \"collision_energy\":  \"CE\", \n",
    "                 \"instrument\": \"Instrument_type\",\n",
    "                 \"precursor_mz\": \"m/z\", \n",
    "                 'precursor_mode': \"Precursor Adduct\",\n",
    "                 \"ccs\": \"CCS_AVG\"\n",
    "                 }\n",
    "\n",
    "ccs_df[\"summary\"] = ccs_df.apply(lambda x: {key: x[name] for key, name in metadata_key.items()}, axis=1)\n",
    "ccs_df.apply(lambda x: x[\"Metabolite\"].add_metadata(x[\"summary\"], setup_encoder, rt_encoder), axis=1)\n",
    "ccs_df.apply(lambda x: x[\"Metabolite\"].set_loss_weight(x[\"loss_weight\"]), axis=1)\n",
    "ccs_df.apply(lambda x: x[\"Metabolite\"].set_precursor_positive(x[\"Precursor Adduct\"]), axis=1)\n",
    "ccs_df.apply(lambda x: x[\"Metabolite\"].set_ring_count(x[\"ring_count\"]), axis=1)\n",
    "ccs_df.apply(lambda x: x[\"Metabolite\"].set_presence_rare_elements(x[\"rare_element_included\"]), axis=1)\n",
    "ccs_df.apply(lambda x: x[\"Metabolite\"].set_elem_distr_vec(x[\"abs_elem_distr_vec\"]), axis=1)\n",
    "\n",
    "ccs_df[\"geo_data\"] = ccs_df[\"Metabolite\"].apply(lambda x: x.as_geometric_data(with_labels=False, ccs_only=True).to(dev)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_molecular_weight(summary_dict):\n",
    "    \"\"\"\n",
    "    Extracts the 'molecular_weight' value from a dictionary.\n",
    "\n",
    "    Args:\n",
    "        summary_dict (dict): A dictionary containing molecular weight information.\n",
    "\n",
    "    Returns:\n",
    "        float: The molecular weight, or None if the key is not found.\n",
    "    \"\"\"\n",
    "    if isinstance(summary_dict, dict):  # Check if the input is a dictionary\n",
    "        return summary_dict.get('molecular_weight')  # Use .get() to avoid KeyError\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "ccs_df[\"weights\"] = ccs_df[\"summary\"].apply(extract_molecular_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [item.weight for item in ccs_df[\"geo_data\"]]\n",
    "\n",
    "ccs_df[\"weights\"] = weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df[\"Precursor Adduct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    data=ccs_df, x='weights', y='CCS_AVG', hue='Precursor Adduct', palette='tab10'\n",
    ")\n",
    "plt.xlabel(\"Molecular Weight\")\n",
    "plt.ylabel(\"CCS (Collision Cross Section)\")\n",
    "plt.title(\"Weight vs. CCS\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Clusters\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Standardize the data\n",
    "X = ccs_df[['weights', 'CCS_AVG']].values\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# DBSCAN clustering\n",
    "clustering = DBSCAN(eps=0.3, min_samples=10).fit(X_scaled)\n",
    "ccs_df['cluster'] = clustering.labels_  # -1 is noise, 0/1 are clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    data=ccs_df, x='weights', y='CCS_AVG', hue='cluster', palette='tab10'\n",
    ")\n",
    "plt.xlabel(\"Molecular Weight\")\n",
    "plt.ylabel(\"CCS (Collision Cross Section)\")\n",
    "plt.title(\"Weight vs. CCS\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "\n",
    "upper_cluster = ccs_df[ccs_df['cluster'] == 1] # has higher CCS values on average\n",
    "bottom_cluster = ccs_df[ccs_df['cluster'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ring_count, rare_element included\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.boxplot(data=ccs_df, x='cluster', y='ring_count', palette='Set2')\n",
    "plt.title('Ring Count Distribution by Cluster')\n",
    "plt.ylabel('Ring Count')\n",
    "plt.xlabel('Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Cluster 0 (bottom cluster) seems to have more rings compared to cluster 1 (upper cluster). This makes sense since molecules \n",
    "# with more rings tend to be more compact\n",
    "\n",
    "# print(upper_cluster['rare_element_included'].mean()) \n",
    "# print(bottom_cluster['rare_element_included'].mean())\n",
    "\n",
    "# 27.6% of the molecules in upper cluster include rare elements compared to 32% in bottom cluster.\n",
    "\n",
    "ring_upper = upper_cluster['ring_count'].dropna()\n",
    "ring_bottom = bottom_cluster['ring_count'].dropna()\n",
    "\n",
    "# Perform t-test for ring count\n",
    "t_stat, p_value = ttest_ind(ring_upper, ring_bottom, equal_var=False)\n",
    "\n",
    "print(f\"Ring count t-test: t = {t_stat:.3f}, p = {p_value}\")    # ~0\n",
    "\n",
    "# Perform chi-square test for rare element existence\n",
    "# Build contingency table\n",
    "table = pd.crosstab(ccs_df['cluster'], ccs_df['rare_element_included'])\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(table)\n",
    "\n",
    "print(f\"Rare element chi-square test: chi2 = {chi2:.3f}, p = {p}\")  # 0.003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ring_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Prediction</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiora = SimulationFramework(None, dev=dev, with_RT=True, with_CCS=True)\n",
    "fiora.pred_all(ccs_df, base_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df['CCS_pred'] = ccs_df[\"Metabolite\"].apply(lambda x: x.CCS_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df['CCS_pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure MSE of predicted and experimental values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = 0\n",
    "\n",
    "mse = ((ccs_df['CCS_AVG'] - ccs_df['CCS_pred'].apply(lambda x: x.item()))**2).mean()\n",
    "print(mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Experimental CCS vs. Predicted CCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(ccs_df['CCS_AVG'], ccs_df['CCS_pred'].apply(lambda x: x.item()), alpha=0.7, label=\"Data points\") #alpha:transparency\n",
    "plt.plot([ccs_df['CCS_AVG'].min(), ccs_df['CCS_AVG'].max()],\n",
    "         [ccs_df['CCS_AVG'].min(), ccs_df['CCS_AVG'].max()],\n",
    "         color='red', linestyle='--', label=\"Ideal Prediction\")\n",
    "\n",
    "plt.xlabel(\"Experimental CCS (CCS_AVG)\")\n",
    "plt.ylabel(\"Predicted CCS (CCS_pred)\")\n",
    "plt.title(\"Comparison of Experimental and Predicted CCS\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "residuals = ccs_df['CCS_pred'].apply(lambda x: x.item()) - ccs_df['CCS_AVG']\n",
    "plt.scatter(ccs_df['CCS_AVG'], residuals, alpha=0.7)\n",
    "plt.axhline(0, color='red', linestyle='--', label=\"Zero Error Line\")\n",
    "\n",
    "plt.xlabel(\"Experimental CCS (CCS_AVG)\")\n",
    "plt.ylabel(\"Residuals (Predicted - Experimental)\")\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "\n",
    "plt.xlabel(\"Residuals (Predicted - Experimental)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Residuals Distribution\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Berechnung des R²-Werts\n",
    "r2 = r2_score(ccs_df['CCS_AVG'], ccs_df['CCS_pred'].apply(lambda x: x.item()))\n",
    "print(f\"R²-Wert: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df['error'] = (ccs_df['CCS_AVG'] - ccs_df['CCS_pred'].apply(lambda x: x.item())).abs()\n",
    "\n",
    "# 100 biggest deviation\n",
    "largest_errors_df = ccs_df.nlargest(100, 'error')\n",
    "print(\"Größte Abweichungen:\")\n",
    "print(largest_errors_df[['CCS_AVG', 'CCS_pred', 'error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a very interpretable plot\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# ccs_pred_values = largest_errors_df['CCS_pred'].apply(lambda x: x.item())\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Scatter plot of the CCS_AVG vs. CCS_pred\n",
    "# sns.scatterplot(data=largest_errors_df, x='CCS_AVG', y=ccs_pred_values)\n",
    "# plt.plot([largest_errors_df['CCS_AVG'].min(), largest_errors_df['CCS_AVG'].max()],\n",
    "#          [largest_errors_df['CCS_AVG'].min(), largest_errors_df['CCS_AVG'].max()],\n",
    "#          color='red', linestyle='--', label=\"Ideal Prediction (y = x)\")\n",
    "# # Titles and labels\n",
    "# plt.title(\"100 Biggest Errors Between CCS_AVG and CCS_pred\")\n",
    "# plt.xlabel(\"CCS_AVG\")\n",
    "# plt.ylabel(\"CCS_pred\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R² for M+H and M-H seperately\n",
    "if not debug_mode:\n",
    "    for mode in ['[M+H]+', '[M-H]-']:\n",
    "        mode_df = ccs_df[ccs_df['Precursor Adduct'] == mode]\n",
    "        r2_mode = r2_score(mode_df['CCS_AVG'], mode_df['CCS_pred'].apply(lambda x: x.item()))\n",
    "        print(f\"R² for {mode}: {r2_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Train/Test/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "def train_val_test_split(keys, test_size=0.1, val_size=0.1, rseed=seed):\n",
    "    temp_keys, test_keys = train_test_split(keys, test_size=test_size, random_state=rseed)\n",
    "    adjusted_val_size = val_size / (1 - test_size)\n",
    "    train_keys, val_keys = train_test_split(temp_keys, test_size=adjusted_val_size, random_state=rseed)\n",
    "    \n",
    "    return train_keys, val_keys, test_keys\n",
    "\n",
    "group_ids = ccs_df[\"group_id\"].astype(int)\n",
    "keys = np.unique(group_ids)\n",
    "example_not_in_test_split = True\n",
    "\n",
    "train, val, test = train_val_test_split(keys, rseed=seed)\n",
    "ccs_df[\"datasplit\"] = ccs_df[\"group_id\"].apply(lambda x: 'train' if x in train else 'validation' if x in val else 'test' if x in test else 'VALUE ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = ccs_df[ccs_df['datasplit'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ccs_df[ccs_df['datasplit'] == 'train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debug_mode:\n",
    "    for mode in ['[M+H]+', '[M-H]-']:\n",
    "        mode_df = test_df[test_df['Precursor Adduct'] == mode]\n",
    "        r2_mode = r2_score(mode_df['CCS_AVG'], mode_df['CCS_pred'].apply(lambda x: x.item()))\n",
    "        print(f\"R² for {mode} (Test Set): {r2_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Compared to Carbon-Ratio\n",
    "\n",
    "test_df['carbon_ratios'] = test_df[\"Metabolite\"].apply(lambda x: x.element_distribution['C'])\n",
    "\n",
    "test_df[\"carbon_bin\"] = pd.cut(test_df[\"carbon_ratios\"], bins=10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=test_df, x=\"carbon_bin\", y=\"error\")\n",
    "plt.title(\"Prediction Error by Carbon Ratio Bins\", fontsize=14)\n",
    "plt.xlabel(\"Carbon Ratio Bins\", fontsize=12)\n",
    "plt.ylabel(\"Error\", fontsize=12)\n",
    "plt.xticks(rotation=45, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiora.MOL.constants\n",
    "\n",
    "def is_element_included(element, metabolite):    # element as symbol\n",
    "    if element in metabolite.node_elements:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def rare_element_included(metabolite):\n",
    "    rare_elements = fiora.MOL.constants.RARE_ELEMENTS\n",
    "    included = False\n",
    "    for element in rare_elements:\n",
    "        included = included or is_element_included(element, metabolite)\n",
    "    return included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if there is any difference in rare_element_included ratios between filtered_df vs. ccs_df\n",
    "# filtered_df['rare_element_included'] = filtered_df[\"Metabolite\"].apply(lambda x : rare_element_included(x))\n",
    "\n",
    "# # Calculate the ratio of True values in each DataFrame\n",
    "# ccs_rare_ratio = ccs_df['rare_element_included'].mean()\n",
    "# filtered_rare_ratio = filtered_df['rare_element_included'].mean()\n",
    "\n",
    "# print(f\"Ratio of rare element included in full dataset: {ccs_rare_ratio:.4f}\")\n",
    "# print(f\"Ratio of rare element included in filtered dataset: {filtered_rare_ratio:.4f}\")\n",
    "\n",
    "# ccs_df['carbon_ratios'] = ccs_df[\"Metabolite\"].apply(lambda x: x.element_distribution['C'])\n",
    "# filtered_df['carbon_ratios'] = filtered_df[\"Metabolite\"].apply(lambda x: x.element_distribution['C'])\n",
    "\n",
    "# ccs_carbon_ratio = ccs_df['carbon_ratios'].mean()\n",
    "# filtered_carbon_ratio = filtered_df['carbon_ratios'].mean()\n",
    "\n",
    "# print(f\"Ratio of carbon in full dataset: {ccs_carbon_ratio:.4f}\")\n",
    "# print(f\"Ratio of carbon in filtered dataset: {filtered_carbon_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Depending on Iod Existence\n",
    "\n",
    "test_df['I_included'] = test_df[\"Metabolite\"].apply(lambda x : is_element_included('I', x))\n",
    "\n",
    "sns.boxplot(data=test_df, x='I_included', y='error')\n",
    "plt.xlabel(\"Element Br Included\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Effect of Element I on Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify Metabolites Based on Murcko Scaffold and Evaluate the Error Rates Depending on These Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Error Depending on Rare Element Existence\n",
    "\n",
    "test_df['rare_element_included'] = test_df[\"Metabolite\"].apply(lambda x : rare_element_included(x))\n",
    "\n",
    "sns.boxplot(data=test_df, x='rare_element_included', y='error')\n",
    "plt.xlabel(\"Rare Element Included\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Effect of Rare Elements on Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from fiora.GNN.Trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torch.nn.MSELoss() \n",
    "\n",
    "# rt_epochs = 500 # 300\n",
    "# rt_batch = 64 #128\n",
    "# rt_lr = 0.005\n",
    "\n",
    "# def train_rt_model(rt_lr=rt_lr, rt_batch=rt_batch, rt_epochs=rt_epochs):        \n",
    "#     y_label = 'compiled_probsALL'\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=rt_lr)\n",
    "#     train_keys, val_keys = ccs_df[ccs_df[\"dataset\"] == \"training\"][\"group_id\"].unique(), ccs_df[ccs_df[\"dataset\"] == \"validation\"][\"group_id\"].unique()\n",
    "#     trainer = Trainer(geo_data, y_tag=y_label, problem_type=\"regression\", train_keys=train_keys, val_keys=val_keys, metric_dict=None, split_by_group=True, seed=seed, device=dev)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 8, factor=0.8, mode = 'min', verbose = True)\n",
    "#     checkpoints = trainer.train(model, optimizer, loss_fn, scheduler=scheduler, batch_size=rt_batch, epochs=rt_epochs, val_every_n_epochs=1, with_CCS=True, with_RT=True, rt_metric=True, masked_validation=False, tag=tag) #, mask_name=\"compiled_validation_maskALL\")   \n",
    "\n",
    "#     return model, checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_dropout_rate(input_dropout=0.5, latent_dropout=0.5)\n",
    "\n",
    "# model, cp = train_rt_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasplit_value_counts = ccs_df['datasplit'].value_counts()\n",
    "print(datasplit_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_df['carbon_ratio'] = ccs_df['Metabolite'].apply(lambda x: x.element_distribution['C'] / len(x.node_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Class PropertyTrainer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch_geometric.loader as geom_loader\n",
    "from torchmetrics import Accuracy, MetricTracker, MetricCollection, Precision, Recall, PrecisionRecallCurve, MeanSquaredError, MeanAbsoluteError, R2Score, PearsonCorrCoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Literal, List, Callable, Any, Dict\n",
    "\n",
    "from fiora.GNN.Trainer import Trainer\n",
    "from fiora.GNN.Datasets import collate_graph_batch, collate_graph_edge_batch\n",
    "from fiora.GNN.Losses import WeightedMSELoss, WeightedMAELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyTrainer(Trainer):\n",
    "    def __init__(self, data: Any, train_val_split: float= 0.8, split_by_group: bool=False, only_training: bool=False, train_keys: List[int]=[], val_keys: List[int]=[], test_keys: List[int]=[], y_tag: str=\"y\", metric_dict: Dict=None, problem_type: Literal[\"classification\", \"regression\", \"softmax_regression\"]=\"classification\", library: Literal[\"standard\", \"geometric\"]=\"geometric\", num_workers: int=0, seed: int=42, device: str=\"cpu\"):\n",
    "        \n",
    "        super().__init__(data, train_val_split, split_by_group, only_training, train_keys, val_keys, test_keys, seed, num_workers, device)\n",
    "        self.y_tag = y_tag\n",
    "        self.problem_type = problem_type\n",
    "\n",
    "        self.data = data \n",
    "         \n",
    "        # Initialize torch metrics based on dictionary \n",
    "        if metric_dict:\n",
    "            self.metrics = {\n",
    "                data_split: MetricTracker(MetricCollection({\n",
    "                        t: M() for t,M in metric_dict.items()       # t: metric name, M() corresponding metric class instance (e.g. torchmetrics.R2Score())\n",
    "                    })).to(device)\n",
    "                for data_split in [\"train\", \"val\", \"test\"]\n",
    "            }\n",
    "        else:\n",
    "            self.metrics = self._get_default_metrics(problem_type)\n",
    "        self.loader_base = geom_loader.DataLoader if library == \"geometric\" else DataLoader\n",
    "    \n",
    "    def _training_loop(self, model, dataloader, optimizer, loss_fn, metrics, property: str=\"ccs\", with_weights=False, with_RT=False, with_CCS=True, rt_metric=False, title=\"\"):\n",
    "        metrics.increment()   \n",
    "\n",
    "        for batch in dataloader: # enumerate  \n",
    "            # print(batch)\n",
    "            # Feed forward\n",
    "            batch = batch.to(dev) # \"cuda:1\"\n",
    "\n",
    "            model.train() # training mode\n",
    "            \n",
    "            y_pred = model(batch)\n",
    "\n",
    "            target = batch[property]#.to(torch.float32)\n",
    "            #target = target.view(-1, 1)\n",
    "            #target = target.to(\"cuda:1\")\n",
    "\n",
    "            # print(\"Predicted shape:\", y_pred[property].shape)\n",
    "            # print(\"Target shape:\", target.shape)\n",
    "\n",
    "            loss = loss_fn(y_pred[property], target)  \n",
    "\n",
    "            # Backpropagate\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "\n",
    "    def _validation_loop(self, model, dataloader, loss_fn, metrics, with_weights=False, with_RT=False,  with_CCS=True, property: str=\"ccs\", rt_metric=False, title=\"Validation\"):\n",
    "        metrics.increment()\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                model.eval()\n",
    "                y_pred = model(batch) # with_CCS = True\n",
    "                loss = loss_fn(y_pred[property], batch[property])\n",
    "                    \n",
    "                metrics(y_pred[property], batch[property]) # call update                      \n",
    "\n",
    "        # End of Validation cycle\n",
    "        stats = metrics.compute()\n",
    "        print(f'\\t{title} RMSE: {torch.sqrt(stats[\"mse\"]):>.4f}')\n",
    "        return stats\n",
    "    \n",
    "    def _test_loop(self, model, dataloader, loss_fn, metrics, property: str=\"ccs\", rt_metric=False):\n",
    "        metrics.increment()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_smiles = []\n",
    "        all_precursors = []\n",
    "        all_ring_counts = []\n",
    "        all_presence_rare_elements = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                model.eval()\n",
    "                y_pred = model(batch)  # Forward pass\n",
    "                loss = loss_fn(y_pred[property], batch[property])\n",
    "\n",
    "                metrics(y_pred[property], batch[property])  # Call update on metrics\n",
    "\n",
    "                all_preds.append(y_pred[property].cpu())\n",
    "                all_targets.append(batch[property].cpu())\n",
    "\n",
    "                if \"smiles\" in batch:\n",
    "                    all_smiles.extend(batch[\"smiles\"])\n",
    "\n",
    "                if \"precursor_positive\" in batch:\n",
    "                    all_precursors.extend(batch[\"precursor_positive\"])\n",
    "                \n",
    "                if \"ring_count\" in batch:\n",
    "                    all_ring_counts.extend(batch[\"ring_count\"])\n",
    "\n",
    "                if \"presence_rare_elements\" in batch:\n",
    "                    all_presence_rare_elements.extend(batch[\"presence_rare_elements\"])\n",
    "                \n",
    "\n",
    "        # End of Test cycle\n",
    "        stats = metrics.compute()\n",
    "        # print(f'\\tTest RMSE: {torch.sqrt(stats[\"mse\"]):>.4f}')\n",
    "\n",
    "        stats[\"predictions\"] = torch.cat(all_preds).numpy()\n",
    "        stats[\"targets\"] = torch.cat(all_targets).numpy()\n",
    "        stats[\"smiles\"] = all_smiles\n",
    "        stats[\"precursor_positive\"] = all_precursors\n",
    "        stats[\"ring_counts\"] = all_ring_counts\n",
    "        stats[\"presence_rare_elements\"] = all_presence_rare_elements\n",
    "\n",
    "        for stat in stats:\n",
    "            stat_name = stat.upper()\n",
    "            if stat_name == \"MSE\":\n",
    "                print(f'\\tTest RMSE: {torch.sqrt(stats[\"mse\"]):>.4f}')\n",
    "            elif stat_name in [\"MSE\", \"MAE\", \"R2\", \"PEARSON\"]:\n",
    "                print(f\"\\tTest {stat_name}: {stats[stat]:.4f}\")\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "    # Training function\n",
    "    def train(self, model, optimizer, loss_fn, scheduler=None, batch_size=16, epochs=2, val_every_n_epochs=1, with_RT=True, with_CCS=True, rt_metric=False, tag=\"\") -> Dict[str, Any]:\n",
    "        \n",
    "        # Set up checkpoint system and model info\n",
    "        self._init_checkpoint_system(save_path=f\"saved_models/checkpoint_{tag}.best.pt\")\n",
    "        model.model_params[\"training_label\"] = self.y_tag\n",
    "        \n",
    "        # Stage data into dataloader\n",
    "        training_loader = self.loader_base(self.data, batch_size=batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "        if not self.only_training:\n",
    "            validation_loader = self.loader_base(self.validation_data, batch_size=batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "        using_weighted_loss_func = isinstance(loss_fn, WeightedMSELoss) | isinstance(loss_fn, WeightedMAELoss)\n",
    "        \n",
    "        # Main loop\n",
    "        for e in range(epochs):\n",
    "            # Training\n",
    "            self._training_loop(model, training_loader, optimizer, loss_fn, self.metrics[\"train\"], title=f'Epoch {e + 1}/{epochs}: ', with_weights=using_weighted_loss_func, with_RT=with_RT, with_CCS=with_CCS, rt_metric=rt_metric)\n",
    "            # Validation\n",
    "            is_val_cycle = not self.only_training and ((e + 1) % val_every_n_epochs == 0)\n",
    "            if is_val_cycle:   \n",
    "                val_stats = self._validation_loop(model, validation_loader, loss_fn, self.metrics[\"val\"], with_weights=using_weighted_loss_func, with_RT=with_RT, with_CCS=with_CCS, rt_metric=rt_metric,  title=\"Validation\")\n",
    "                \n",
    "                # Update checkpoint\n",
    "                if val_stats[\"mse\"].tolist() < self.checkpoint_stats[\"val_loss\"]:\n",
    "                    self._update_checkpoint({\"epoch\": e+1, \"val_loss\": val_stats[\"mse\"].tolist()}, model)\n",
    "                    print(f\"\\t >> Set new checkpoint to epoch {e+1}\")\n",
    "            \n",
    "            # End of epoch: Advance scheduler\n",
    "            if scheduler:\n",
    "                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                    if is_val_cycle:\n",
    "                        scheduler.step(torch.sqrt(val_stats[\"mse\"]))\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "                    \n",
    "        print(\"Finished Training!\")\n",
    "        return self.checkpoint_stats\n",
    "\n",
    "    def test(self, model, loss_fn, batch_size=16, tag=\"\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Test the trained model on the test dataset and compute the RMSE.\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            raise ValueError(\"Model is None. Ensure the model is correctly initialized and passed.\")\n",
    "\n",
    "        # Stage test data into dataloader\n",
    "        test_loader = self.loader_base(self.test_data, batch_size=batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "        # Test loop\n",
    "        print(f\"Testing model: {tag}\")\n",
    "        test_stats = self._test_loop(model, test_loader, loss_fn, self.metrics[\"test\"])\n",
    "\n",
    "        return test_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.data import Dataset\n",
    "\n",
    "# class CCSGeometricDataset(Dataset):\n",
    "#     def __init__(self, data, targets, group_ids, datasplit):\n",
    "#         super().__init__()\n",
    "#         self.data = data  # List of graph data objects\n",
    "#         self.targets = targets  # Corresponding target values (ccs_avg)\n",
    "#         self.group_ids = group_ids\n",
    "#         self.datasplit = datasplit\n",
    "\n",
    "#     def len(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def get(self, idx):\n",
    "#         graph = self.data[idx]  # PyTorch Geometric graph object\n",
    "#         target = self.targets[idx]  # Associated target value\n",
    "#         group_ids = self.group_ids[idx]\n",
    "#         datasplit = self.datasplit[idx]\n",
    "#         graph['ccs'] = target  # Add target as an attribute in the graph\n",
    "#         graph['group_ids'] = group_ids\n",
    "#         graph['datasplit'] = datasplit\n",
    "#         return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and targets\n",
    "# ccs_targets = ccs_df['CCS_AVG'].values\n",
    "# group_ids = ccs_df['group_id'].values\n",
    "# datasplit_values = ccs_df['datasplit'].values\n",
    "\n",
    "# # Create dataset\n",
    "# ccs_dataset = CCSGeometricDataset(data=geo_data, targets=ccs_targets, group_ids=group_ids, datasplit=datasplit_values)\n",
    "\n",
    "# # Create a boolean mask to index the dataset\n",
    "# train_mask = ccs_df['datasplit'] == \"train\"\n",
    "# val_mask = ccs_df['datasplit'] == \"validation\"\n",
    "\n",
    "# # Index using boolean arrays\n",
    "# train_data = ccs_dataset[train_mask.to_numpy()]\n",
    "# val_data = ccs_dataset[val_mask.to_numpy()]\n",
    "\n",
    "# Create DataLoaders\n",
    "# train_loader = geom_loader.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "# val_loader = geom_loader.DataLoader(val_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_data = ccs_df['geo_data'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_data = torch.tensor(geo_data, dtype=torch.float32).to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.0001)\n",
    "train_keys, val_keys, test_keys = ccs_df[ccs_df[\"datasplit\"] == \"train\"][\"group_id\"].unique(), ccs_df[ccs_df[\"datasplit\"] == \"validation\"][\"group_id\"].unique(), ccs_df[ccs_df[\"datasplit\"] == \"test\"][\"group_id\"].unique()\n",
    "\n",
    "trainer_fiora = PropertyTrainer(\n",
    "    data=geo_data,\n",
    "    y_tag='ccs',\n",
    "    problem_type=\"regression\",\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    test_keys=test_keys,\n",
    "    metric_dict=None,\n",
    "    split_by_group=True,\n",
    "    device=dev # \"cuda:1\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "checkpoint = trainer_fiora.train(\n",
    "    model=base_model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=30,\n",
    "    val_every_n_epochs=1,\n",
    "    with_CCS=True,\n",
    "    tag=\"fiora\"\n",
    ")\n",
    "\n",
    "optimizer_lora = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "checkpoint_lora = trainer_fiora.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=30,\n",
    "    val_every_n_epochs=1,\n",
    "    with_CCS=True,\n",
    "    tag=\"fiora\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO THIS TO LOAD MODEL AND TEST THAT MODEL\n",
    "# ALSO FOR DIFFERENT MODELS\n",
    "\n",
    "# model = model.load(checkpoint[\"file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geo_data[0].x)  # Assuming geo_data contains PyTorch Geometric Data objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Linear Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)  # Xavier Initialization\n",
    "        nn.init.zeros_(m.bias)  # Initialize bias to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        # Include model parameters as required by PropertyTrainer\n",
    "        self.model_params = {\n",
    "            \"training_label\": None, # changed to 'ccs' through y_tag in PropertyTrainer\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "        }\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Handle dictionary-style batches\n",
    "        # print(batch)\n",
    "        if isinstance(batch, dict):\n",
    "            x = batch.get(\"data\")\n",
    "            # print(f\"X: {x}\")\n",
    "\n",
    "        # Handle PyTorch Geometric-style batches\n",
    "        elif hasattr(batch, \"weight\") and hasattr(batch, \"precursor_positive\"):\n",
    "            # , \"ring_count\", \"presence_rare_elements\", \"elem_distr_vec\"\n",
    "            attributes = [\"weight\", \"precursor_positive\"]\n",
    "\n",
    "            # Extract attributes if they exist, otherwise use a zero tensor\n",
    "            features = []\n",
    "            \n",
    "            for attr in attributes:\n",
    "                if hasattr(batch, attr):\n",
    "\n",
    "                    value = getattr(batch, attr)\n",
    "\n",
    "                    # if isinstance(value, (list, np.ndarray)):\n",
    "                    #     tensor = torch.tensor(value, dtype=torch.float32).unsqueeze(-1).to(dev)\n",
    "                    # else:\n",
    "                    #     tensor = value.unsqueeze(-1).float().to(dev)\n",
    "                    if isinstance(value, (list, np.ndarray)):\n",
    "                        value = np.array(value)\n",
    "\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        tensor = value.float().to(dev)\n",
    "                    else:\n",
    "                        tensor = torch.tensor(value, dtype=torch.float32).to(dev)\n",
    "\n",
    "                    # Ensure tensor is 2D: [batch_size, feature_dim]\n",
    "                    if tensor.ndim == 1:\n",
    "                        tensor = tensor.unsqueeze(-1)\n",
    "                    elif tensor.ndim > 2:\n",
    "                        tensor = tensor.view(tensor.size(0), -1)\n",
    "\n",
    "                else:\n",
    "                    tensor = torch.zeros_like(batch.weight).unsqueeze(-1).float().to(dev)\n",
    "                    \n",
    "                features.append(tensor)\n",
    "\n",
    "            # Concatenate all features\n",
    "            x = torch.cat(features, dim=-1)\n",
    "\n",
    "            # Ensure the input is a float tensor\n",
    "            x = x.to(dev)#to(torch.float32).to(\"cuda:1\")\n",
    "\n",
    "            # Forward pass\n",
    "            output = self.linear(x)\n",
    "            return {\"ccs\": output}\n",
    "\n",
    "        else:\n",
    "            raise KeyError(\"Batch does not contain the required 'data' or 'weight' & 'precursor_positive' attributes.\")\n",
    "\n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the model to the given filepath.\"\"\"\n",
    "        # GNNCompiler.save(filepath)\n",
    "        # print(f\"State Dict: {self.state_dict()}\")\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load the model's state from a given filepath.\"\"\"\n",
    "        # Load the state dictionary from the saved checkpoint\n",
    "        # GNNCompiler.load(PATH=filepath)\n",
    "        self.load_state_dict(torch.load(filepath, map_location=\"cpu\")) # cuda:1 / cpu\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        \n",
    "# Instantiate the model\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "linear_model = LinearModel(input_dim, output_dim).to(dev)\n",
    "\n",
    "#linear_model.apply(initialize_weights)\n",
    "\n",
    "optimizer_linear = torch.optim.Adam(linear_model.parameters(), lr=0.001)\n",
    "\n",
    "# for param in linear_model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "trainer_linear = PropertyTrainer(\n",
    "    data=geo_data,\n",
    "    y_tag=\"ccs\",\n",
    "    problem_type=\"regression\",\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    test_keys=test_keys,\n",
    "    metric_dict=None,\n",
    "    split_by_group=True,\n",
    "    device=dev, # \"cuda:1\"\n",
    "    library=\"geometric\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# linear_model = linear_model.to(\"cuda:1\")\n",
    "linear_checkpoint = trainer_linear.train(\n",
    "    model=linear_model,\n",
    "    optimizer=optimizer_linear,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=30,\n",
    "    val_every_n_epochs=1,\n",
    "    with_CCS=True,\n",
    "    tag=\"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric as geom\n",
    "optimizer_linear = torch.optim.Adam(linear_model.parameters(), lr=0.001)\n",
    "y_pred = linear_model(geom.data.Batch.from_data_list(geo_data[0:5]))\n",
    "loss = loss_fn(y_pred[\"ccs\"], geom.data.Batch.from_data_list(geo_data[0:5])[\"ccs\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_linear.zero_grad()\n",
    "loss.backward()\n",
    "optimizer_linear.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[\"ccs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom.data.Batch.from_data_list(geo_data[0:5])[\"ccs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# filepath = linear_checkpoint[\"file\"]\n",
    "# print(f\"Checkpoint path: {filepath}\")\n",
    "# print(f\"File exists: {os.path.exists(filepath)}\")\n",
    "# print(f\"File size: {os.path.getsize(filepath) if os.path.exists(filepath) else 'N/A'} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model.save('../../checkpoint_test.pt')\n",
    "# lin_model_test = linear_model.load('../../checkpoint_test.pt')\n",
    "\n",
    "# print(lin_model_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Linear Model from checkpoint\n",
    "\n",
    "linear_model.load(linear_checkpoint[\"file\"])\n",
    "\n",
    "# In Debug Mode\n",
    "# RuntimeError: Invalid magic number; corrupt file?\n",
    "# .pt file size: 19809568 bytes\n",
    "\n",
    "# Not In Debug Mode\n",
    "# No error, linear_model = None\n",
    "# .pt file size: 1600 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MLP Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # First hidden layer with 32 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),         # Second hidden layer with 16 neurons\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim)  # Output layer\n",
    "        )\n",
    "\n",
    "        # Include model parameters as required by PropertyTrainer\n",
    "        self.model_params = {\n",
    "            \"training_label\": None, # changed to 'ccs' through y_tag in PropertyTrainer\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "        }\n",
    "\n",
    "    def forward(self, batch):\n",
    "        if isinstance(batch, dict):\n",
    "            x = batch.get(\"data\")\n",
    "        elif hasattr(batch, \"weight\") and hasattr(batch, \"precursor_positive\"):\n",
    "            # , \"ring_count\", \"presence_rare_elements\", \"elem_distr_vec\"\n",
    "            attributes = [\"weight\", \"precursor_positive\"]\n",
    "\n",
    "            # Extract attributes if they exist, otherwise use a zero tensor\n",
    "            features = []\n",
    "            \n",
    "            for attr in attributes:\n",
    "                if hasattr(batch, attr):\n",
    "                    value = getattr(batch, attr)\n",
    "                    if isinstance(value, (list, np.ndarray)):\n",
    "                        value = np.array(value)\n",
    "\n",
    "                    if isinstance(value, torch.Tensor):\n",
    "                        tensor = value.float().to(dev)\n",
    "                    else:\n",
    "                        tensor = torch.tensor(value, dtype=torch.float32).to(dev)\n",
    "\n",
    "                    # Ensure tensor is 2D: [batch_size, feature_dim]\n",
    "                    if tensor.ndim == 1:\n",
    "                        tensor = tensor.unsqueeze(-1)\n",
    "                    elif tensor.ndim > 2:\n",
    "                        tensor = tensor.view(tensor.size(0), -1)\n",
    "                else:\n",
    "                    tensor = torch.zeros_like(batch.weight).unsqueeze(-1).float().to(dev)\n",
    "                features.append(tensor)\n",
    "\n",
    "            # Concatenate all features\n",
    "            x = torch.cat(features, dim=-1)\n",
    "\n",
    "            # Ensure the input is a float tensor\n",
    "            x = x.to(dev)\n",
    "        else:\n",
    "            raise KeyError(\"Batch does not contain 'data' or 'weight' & 'precursor_positive' attributes.\")\n",
    "\n",
    "        return {\"ccs\": self.layers(x)}\n",
    "\n",
    "    def save(self, filepath):\n",
    "        torch.save(self.state_dict(), filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.load_state_dict(torch.load(filepath, map_location=\"cpu\")) # cuda:1 / cpu\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "# Instantiate Model\n",
    "input_dim = 2\n",
    "output_dim = 1  \n",
    "mlp_model = MLPModel(input_dim, output_dim).to(dev)\n",
    "\n",
    "# print(f\"MLP Model: {mlp_model}\")\n",
    "\n",
    "optimizer_mlp = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "trainer_mlp = PropertyTrainer(\n",
    "    data=geo_data,\n",
    "    y_tag=\"ccs\",\n",
    "    problem_type=\"regression\",\n",
    "    train_keys=train_keys,\n",
    "    val_keys=val_keys,\n",
    "    test_keys=test_keys,\n",
    "    metric_dict=None,\n",
    "    split_by_group=True,\n",
    "    device=dev,\n",
    "    library=\"geometric\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_checkpoint = trainer_mlp.train(\n",
    "    model=mlp_model,\n",
    "    optimizer=optimizer_mlp,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=30,\n",
    "    val_every_n_epochs=1,\n",
    "    with_CCS=True,\n",
    "    tag=\"mlp\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = mlp_model.load(mlp_checkpoint[\"file\"])\n",
    "# print(f\"MLP Model: {mlp_model}\")\n",
    "\n",
    "# filepath = mlp_checkpoint[\"file\"]\n",
    "# print(f\"Checkpoint path: {filepath}\")\n",
    "# print(f\"File exists: {os.path.exists(filepath)}\")\n",
    "# print(f\"File size: {os.path.getsize(filepath) if os.path.exists(filepath) else 'N/A'} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLP Model from checkpoint\n",
    "\n",
    "mlp_model.load(mlp_checkpoint[\"file\"])\n",
    "\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fiora Model from checkpoint\n",
    "\n",
    "base_model.load(checkpoint[\"file\"])  # model = fiora_model\n",
    "print(base_model)\n",
    "# filepath = checkpoint[\"file\"]\n",
    "# print(f\"Checkpoint path: {filepath}\")\n",
    "# print(f\"File exists: {os.path.exists(filepath)}\")\n",
    "# print(f\"File size: {os.path.getsize(filepath) if os.path.exists(filepath) else 'N/A'} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA Fiora Model from checkpoint\n",
    "model.load(checkpoint_lora[\"file\"])\n",
    "print(f\"LoRA Model: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stats_linear = trainer.test(model=linear_model, loss_fn=loss_fn, batch_size=16, tag=\"linear_model\")\n",
    "# print(f\"Test RMSE: {torch.sqrt(test_stats_linear['mse']):>.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiora Model Test\n",
    "fiora_model = base_model.to(dev)\n",
    "lora_fiora_model = model.to(dev)\n",
    "\n",
    "# Residual-Werte, plotten\n",
    "\n",
    "test_stats_fiora = trainer_fiora.test(fiora_model, loss_fn, 16, \"fiora_test\")\n",
    "# print(f\"Test MSE: {(test_stats_fiora['mse']):>.4f}\")\n",
    "test_stats_lora_fiora = trainer_fiora.test(lora_fiora_model, loss_fn, 16, \"lora_fiora_test\")\n",
    "\n",
    "test_stats_linear = trainer_linear.test(linear_model, loss_fn, 16, \"linear_test\")\n",
    "\n",
    "test_stats_mlp = trainer_mlp.test(mlp_model, loss_fn, 16, \"mlp_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Visualization of Test Results</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find global min and max for consistent axes\n",
    "all_targets = np.concatenate([test_stats_fiora[\"targets\"], test_stats_linear[\"targets\"], test_stats_mlp[\"targets\"]])\n",
    "all_predictions = np.concatenate([test_stats_fiora[\"predictions\"], test_stats_linear[\"predictions\"], test_stats_mlp[\"predictions\"]])\n",
    "\n",
    "global_min = min(all_targets.min(), all_predictions.min())\n",
    "global_max = max(all_targets.max(), all_predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Stats Comparison of Models, Barplot\n",
    "\n",
    "def compare_models_barplot(model_names, test_stats, metric): # give model_name and test_stats as arrays with the correct order\n",
    "    metric = metric.lower()\n",
    "\n",
    "    if not all(metric in stat for stat in test_stats) and metric != \"rmse\":\n",
    "        raise ValueError(f\"Metric '{metric}' not found in test stats dictionaries.\")\n",
    "\n",
    "    if metric == 'rmse':\n",
    "        scores = [stat['mse'].cpu().numpy() for stat in test_stats]  # loops test stat dictionaries and extracts the input metric\n",
    "        scores = np.sqrt(scores)\n",
    "    else:\n",
    "        scores = [stat[metric].cpu().numpy() for stat in test_stats]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(model_names, scores, color=['blue', 'orange', 'green']) \n",
    "\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(metric.upper())\n",
    "    plt.title(f\"Model Comparison: {metric.upper()}\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models_barplot([\"LoRA + Fiora\", \"Fiora\", \"Linear\", \"MLP\"], [test_stats_lora_fiora, test_stats_fiora, test_stats_linear, test_stats_mlp], \"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_test_stats_lora_fiora = test_stats_lora_fiora.copy()\n",
    "filtered_test_stats_fiora = test_stats_fiora.copy()\n",
    "filtered_test_stats_linear = test_stats_linear.copy()\n",
    "filtered_test_stats_mlp = test_stats_mlp.copy()\n",
    "\n",
    "# Delete unwanted keys\n",
    "for key in ['mae', 'mse', 'r2', 'pearson']:\n",
    "    filtered_test_stats_lora_fiora.pop(key, None)\n",
    "    filtered_test_stats_fiora.pop(key, None)\n",
    "    filtered_test_stats_linear.pop(key, None)\n",
    "    filtered_test_stats_mlp.pop(key, None)\n",
    "\n",
    "# Create a DataFrame that includes prediction and target CCS, as well as smiles\n",
    "test_df_lora_fiora = pd.DataFrame({key: np.array(value).flatten() for key, value in filtered_test_stats_lora_fiora.items()})\n",
    "test_df_fiora = pd.DataFrame({key: np.array(value).flatten() for key, value in filtered_test_stats_fiora.items()})\n",
    "test_df_linear = pd.DataFrame({key: np.array(value).flatten() for key, value in filtered_test_stats_linear.items()})\n",
    "test_df_mlp = pd.DataFrame({key: np.array(value).flatten() for key, value in filtered_test_stats_mlp.items()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_fiora['rare_element_included'] = test_df_fiora['smiles'].apply(rare_element_included_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_fiora['residuals'] = (test_df_fiora['predictions'] - test_df_fiora['targets']).abs()\n",
    "\n",
    "test_stats_lora_fiora['residuals'] = (test_df_lora_fiora['predictions'] - test_df_lora_fiora['targets']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_fiora.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_lora_fiora.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot for residuals grouped by rare_element_included column\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='rare_element_included', y='residuals', data=test_df_fiora)\n",
    "\n",
    "plt.xlabel('Rare Element Included')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals by Rare Element Inclusion (Fiora)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rare_elements_in_smiles(smiles, element=None):\n",
    "    rare_elements = fiora.MOL.constants.RARE_ELEMENTS\n",
    "\n",
    "    if element:\n",
    "        # Count only the specified element\n",
    "        return int(bool(re.search(rf'\\b{element}\\b', smiles)))  # Returns 1 if present, 0 otherwise\n",
    "    else:\n",
    "        # Count all rare elements\n",
    "        return sum(1 for elem in rare_elements if re.search(rf'\\b{elem}\\b', smiles))\n",
    "\n",
    "# Add a new column 'rare_element_count' to store the count of rare elements in each smile string\n",
    "test_df_fiora['rare_element_count'] = test_df_fiora['smiles'].apply(count_rare_elements_in_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot for residuals based on the number of rare elements\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='rare_element_count', y='residuals', data=test_df_fiora)\n",
    "\n",
    "plt.xlabel('Count of Rare Elements')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals by Count of Rare Elements (Fiora)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_error_smiles = test_df_fiora.nlargest(10, 'residuals')['smiles']\n",
    "\n",
    "molecule_images = []\n",
    "\n",
    "# Create a molecule image for each smiles\n",
    "for smiles in top_10_error_smiles:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    img = Draw.MolToImage(mol)\n",
    "    molecule_images.append(img)\n",
    "\n",
    "# Plot the images in a grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()  # Flatten the 2x5 grid of axes\n",
    "\n",
    "# Plot each molecule image in the grid\n",
    "for i, img in enumerate(molecule_images):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axes\n",
    "\n",
    "plt.suptitle('Top 10 Molecules with Highest Residuals (Fiora)', fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10_error_smiles = test_df_fiora.nsmallest(10, 'residuals')['smiles']\n",
    "\n",
    "molecule_images = []\n",
    "\n",
    "# Create a molecule image for each smiles\n",
    "for smiles in bottom_10_error_smiles:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    img = Draw.MolToImage(mol)\n",
    "    molecule_images.append(img)\n",
    "\n",
    "# Plot the images in a grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()  # Flatten the 2x5 grid of axes\n",
    "\n",
    "# Plot each molecule image in the grid\n",
    "for i, img in enumerate(molecule_images):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axes\n",
    "\n",
    "plt.suptitle('Bottom 10 Molecules with Highest Residuals (Fiora)', fontsize=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_element_ratio_all = test_df_fiora['rare_element_count'].mean()\n",
    "print(f\"Rare element count mean for all {len(test_df_fiora)} rows: {rare_element_ratio_all:.4f}\")\n",
    "\n",
    "top_30_errors = test_df_fiora.nlargest(30, 'residuals')['rare_element_count']\n",
    "\n",
    "# Calculate the ratio of rare elements included in the top 100 errors\n",
    "rare_element_ratio_top_30 = top_30_errors.mean()\n",
    "print(f\"Rare element count mean for top 30 errors: {rare_element_ratio_top_30:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "element_counts = Counter()\n",
    "\n",
    "for _, row in ccs_df.iterrows():\n",
    "    elem_distr = row[\"Metabolite\"].calc_abs_elem_distr()\n",
    "    element_counts.update(elem_distr)\n",
    "\n",
    "element_df = pd.DataFrame.from_dict(element_counts, orient='index', columns=['Count'])\n",
    "element_df = element_df.sort_values(by='Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if pos and neg have similar R2 scores\n",
    "\n",
    "positive_df = test_df_fiora[test_df_fiora[\"precursor_positive\"] == True]\n",
    "negative_df = test_df_fiora[test_df_fiora[\"precursor_positive\"] == False]\n",
    "\n",
    "# Compute R² Score for each group\n",
    "r2_positive = r2_score(positive_df[\"targets\"], positive_df[\"predictions\"])\n",
    "r2_negative = r2_score(negative_df[\"targets\"], negative_df[\"predictions\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"R² Score for Positive Precursor Adduct: {r2_positive:.4f}\")\n",
    "print(f\"R² Score for Negative Precursor Adduct: {r2_negative:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight vs. CCS (target) of test set\n",
    "# extract weight from SMILES\n",
    "\n",
    "test_df_fiora[\"Metabolite\"] = test_df_fiora[\"smiles\"].apply(Metabolite)\n",
    "test_df_fiora[\"Weight\"] = test_df_fiora[\"Metabolite\"].apply(lambda x: x.ExactMolWeight)\n",
    "\n",
    "test_df_fiora[\"MSE\"] = (test_df_fiora[\"targets\"] - test_df_fiora[\"predictions\"]) ** 2\n",
    "test_df_linear[\"MSE\"] = (test_df_linear[\"targets\"] - test_df_linear[\"predictions\"]) ** 2\n",
    "test_df_mlp[\"MSE\"] = (test_df_mlp[\"targets\"] - test_df_mlp[\"predictions\"]) ** 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Set: CCS vs. Weight\n",
    "\n",
    "top_100_mse_fiora = test_df_fiora.nlargest(100, 'MSE')\n",
    "top_100_mse_linear = test_df_linear.nlargest(100, 'MSE')\n",
    "top_100_mse_mlp = test_df_mlp.nlargest(100, 'MSE')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_fiora,\n",
    "    x=\"Weight\",\n",
    "    y=\"targets\",\n",
    "    alpha=0.5,\n",
    "    color=\"blue\",\n",
    "    label=\"Other Points\"\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=top_100_mse_fiora,\n",
    "    x=\"Weight\",\n",
    "    y=\"targets\",\n",
    "    alpha=0.8,\n",
    "    color=\"red\",\n",
    "    label=\"Top 100 MSE (Fiora Preds)\"\n",
    ")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"CCS\")\n",
    "plt.title(\"Scatter Plot of CCS vs. Molecular Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiora\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_fiora,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    alpha=0.4,\n",
    "    color=\"blue\",\n",
    "    label=\"Other Points\"\n",
    ")\n",
    "# Plot the top 100 MSE points in red\n",
    "sns.scatterplot(\n",
    "    data=top_100_mse_fiora,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    alpha=0.8,\n",
    "    color=\"red\",\n",
    "    label=\"Top 100 MSE (Fiora Preds)\"\n",
    ")\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot with Top 100 MSE Points in Red (Fiora)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Linear\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_linear,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    alpha=0.4,\n",
    "    color=\"blue\",\n",
    "    label=\"Other Points\"\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=top_100_mse_linear,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    alpha=0.8,\n",
    "    color=\"red\",\n",
    "    label=\"Top 100 MSE (Linear Preds)\"\n",
    ")\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot with Top 100 MSE Points in Red (Linear)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# MLP\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_mlp,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    alpha=0.4,\n",
    "    color=\"blue\",\n",
    "    label=\"Other Points\"\n",
    ")\n",
    "sns.scatterplot(\n",
    "    data=top_100_mse_mlp,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    alpha=0.8,\n",
    "    color=\"red\",\n",
    "    label=\"Top 100 MSE (MLP Preds)\"\n",
    ")\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot with Top 100 MSE Points in Red (MLP)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiora Ground Truth vs. Prediction\n",
    "\n",
    "plt.scatter(test_stats_fiora[\"targets\"], test_stats_fiora[\"predictions\"], alpha=0.4)\n",
    "plt.plot([global_min, global_max], [global_min, global_max], 'r--')\n",
    "plt.xlim(global_min, global_max)\n",
    "plt.ylim(global_min, global_max)\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Ground Truth vs. Prediction for Fiora Predictions\")\n",
    "plt.show()\n",
    "\n",
    "# LoRA + Fiora Ground Truth vs. Prediction\n",
    "\n",
    "plt.scatter(test_stats_lora_fiora[\"targets\"], test_stats_lora_fiora[\"predictions\"], alpha=0.4)\n",
    "plt.plot([global_min, global_max], [global_min, global_max], 'r--')\n",
    "plt.xlim(global_min, global_max)\n",
    "plt.ylim(global_min, global_max)\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Ground Truth vs. Prediction for LoRA + Fiora Predictions\")\n",
    "plt.show()\n",
    "\n",
    "# Linear Model Ground Truth vs. Prediction\n",
    "\n",
    "plt.scatter(test_stats_linear[\"targets\"], test_stats_linear[\"predictions\"], alpha=0.4)\n",
    "plt.plot([global_min, global_max], [global_min, global_max], 'r--')\n",
    "plt.xlim(global_min, global_max)\n",
    "plt.ylim(global_min, global_max)\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Ground Truth vs. Prediction for Linear Model Predictions\")\n",
    "plt.show()\n",
    "\n",
    "# MLP Ground Truth vs. Prediction\n",
    "\n",
    "plt.scatter(test_stats_mlp[\"targets\"], test_stats_mlp[\"predictions\"], alpha=0.4)\n",
    "plt.plot([global_min, global_max], [global_min, global_max], 'r--')\n",
    "plt.xlim(global_min, global_max)\n",
    "plt.ylim(global_min, global_max)\n",
    "plt.xlabel(\"Ground Truth\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Ground Truth vs. Prediction for MLP Model Predictions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiora\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_fiora,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    hue=\"precursor_positive\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.plot([global_min, global_max], [global_min, global_max], color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "plt.xlabel(\"Targets\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot of Targets vs. Predictions (Fiora)\")\n",
    "plt.legend(title=\"Precursor Positive\")\n",
    "plt.show()\n",
    "\n",
    "# LoRA + Fiora\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_lora_fiora,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    hue=\"precursor_positive\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.plot([global_min, global_max], [global_min, global_max], color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "plt.xlabel(\"Targets\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot of Targets vs. Predictions (LoRA + Fiora)\")\n",
    "plt.legend(title=\"Precursor Positive\")\n",
    "plt.show()\n",
    "\n",
    "# Linear\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_linear,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    hue=\"precursor_positive\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.plot([global_min, global_max], [global_min, global_max], color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "plt.xlabel(\"Targets\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot of Targets vs. Predictions (Linear)\")\n",
    "plt.legend(title=\"Precursor Positive\")\n",
    "plt.show()\n",
    "\n",
    "# MLP\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=test_df_mlp,\n",
    "    x=\"targets\",\n",
    "    y=\"predictions\",\n",
    "    hue=\"precursor_positive\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.plot([global_min, global_max], [global_min, global_max], color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "plt.xlabel(\"Targets\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "plt.title(\"Scatter Plot of Targets vs. Predictions (MLP)\")\n",
    "plt.legend(title=\"Precursor Positive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 2 Molecules with Similar Weights but Different CCS Values\n",
    "\n",
    "threshold_weight = 1\n",
    "\n",
    "def find_rows(df):\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i + 1, len(df)):\n",
    "            if abs(df['Weight'][i] - df['Weight'][j]) < 1 and abs(df['targets'][i] - df['targets'][j]) > 20:\n",
    "                return (i,j)\n",
    "    return None\n",
    "\n",
    "# Find the rows\n",
    "result_rows = find_rows(test_df_fiora)\n",
    "\n",
    "# Print the result\n",
    "if result_rows is not None:\n",
    "    print(result_rows)  # 1,230\n",
    "else:\n",
    "    print(\"No such rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_molecule = test_df_fiora.iloc[result_rows[0]].copy()\n",
    "second_molecule = test_df_fiora.iloc[result_rows[1]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Example Molecules that have similar weight but different CCS\n",
    "\n",
    "first_smiles = first_molecule[\"smiles\"]\n",
    "second_smiles = second_molecule[\"smiles\"]\n",
    "\n",
    "first_weight = first_molecule[\"Weight\"]\n",
    "second_weight = second_molecule[\"Weight\"]\n",
    "\n",
    "first_ccs = first_molecule[\"targets\"]\n",
    "second_ccs = second_molecule[\"targets\"] \n",
    "\n",
    "first_pred = first_molecule[\"predictions\"]\n",
    "second_pred = second_molecule[\"predictions\"]\n",
    "first_mse = (first_ccs - first_pred) ** 2\n",
    "second_mse = (second_ccs - second_pred) ** 2\n",
    "\n",
    "mol_images = []\n",
    "\n",
    "mol1 = Chem.MolFromSmiles(first_smiles)\n",
    "img1 = Draw.MolToImage(mol1)\n",
    "mol_images.append(img1)\n",
    "\n",
    "mol2 = Chem.MolFromSmiles(second_smiles)\n",
    "img2 = Draw.MolToImage(mol2)\n",
    "mol_images.append(img2)\n",
    "\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))  # 1 row, 2 columns\n",
    "\n",
    "for i, ax in enumerate(axes):  # No need to use flatten()\n",
    "    ax.imshow(mol_images[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Annotate with CCS, Weight, and MSE\n",
    "    if i == 0:\n",
    "        text = f\"CCS: {first_ccs:.4f} | CCS Pred: {first_pred:.4f} | Weight: {first_weight:.4f} | MSE: {first_mse:.4f}\" \n",
    "        # text = f\"CCS: {first_ccs:.4f} | Weight: {first_weight:.4f}\" \n",
    "    else:\n",
    "        text = f\"CCS: {second_ccs:.4f} | CCS Pred: {second_pred:.4f} | Weight: {second_weight:.4f} | MSE: {second_mse:.4f}\"\n",
    "        # text = f\"CCS: {second_ccs:.4f} | Weight: {second_weight:.4f}\" \n",
    "\n",
    "    # Add the text below each image\n",
    "    ax.text(0.5, -0.05, text, ha='center', va='top', transform=ax.transAxes, fontsize=15)\n",
    "\n",
    "plt.tight_layout(pad=0.9, w_pad=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_fiora[\"precursor_positive\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_fiora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ring_count vs. Residual (Fiora)\n",
    "\n",
    "ring_counts = [tensor.item() for tensor in test_stats_fiora[\"ring_counts\"]]\n",
    "\n",
    "predictions = test_stats_fiora[\"predictions\"].flatten()\n",
    "targets = test_stats_fiora[\"targets\"].flatten()\n",
    "residuals = np.abs(predictions - targets)\n",
    "\n",
    "ring_count_res_df = pd.DataFrame({'Ring Count': ring_counts, 'Residuals': residuals})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Ring Count', y='Residuals', data=ring_count_res_df)\n",
    "\n",
    "plt.xlabel(\"Ring Counts\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Boxplot of Residuals vs. Ring Counts\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_fiora.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters in test_df_fiora\n",
    "\n",
    "# Standardize the data\n",
    "X = ccs_df[['weights', 'CCS_AVG']].values\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# DBSCAN clustering\n",
    "clustering = DBSCAN(eps=0.3, min_samples=10).fit(X_scaled)\n",
    "ccs_df['cluster'] = clustering.labels_  # -1 is noise, 0/1 are clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_results': [], 'superclass_results': [], 'pathway_results': ['Shikimates and Phenylpropanoids'], 'isglycoside': False, 'fp1': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'fp2': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# TODO: find superclasses of metabolites and analyze them according to superclass clusters\n",
    "\n",
    "# ClassyFire\n",
    "# test_df_fiora[\"group_id\"] = test_df_fiora[\"smiles\"].astype(\"category\").cat.codes\n",
    "# test_df_fiora.drop_duplicates(\"group_id\", keep=\"first\")[[\"group_id\", \"smiles\"]].to_csv(f\"../../data/ccs/benchmarking/classyfire_input.csv\", header=None, sep=\" \", index=False)\n",
    "\n",
    "#NPClassifier (doesn't work)\n",
    "\n",
    "import requests\n",
    "\n",
    "def classify_np(smiles):\n",
    "    try:\n",
    "        response = requests.get(f\"https://structure.gnps2.org/classyfire?smiles={smiles}\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('superclass', None)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "superclasses = []\n",
    "for i, s in enumerate(test_df_fiora['smiles']):\n",
    "    if pd.isna(s):\n",
    "        print(\"smiles NA\")\n",
    "        superclasses.append(None)\n",
    "        continue\n",
    "    superclass = classify_np(s)\n",
    "    superclasses.append(superclass)\n",
    "\n",
    "test_df_fiora['superclass'] = superclasses\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
